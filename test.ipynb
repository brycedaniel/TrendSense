{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSV file with selected columns and no empty rows saved as Combined_Total_Clean_3.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the original CSV file\n",
    "file_path = \"Combined_Clean_1.csv\"  # Replace with the actual file path if needed\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the data is sorted by 'ticker' and 'publish_date'\n",
    "df['publish_date'] = pd.to_datetime(df['publish_date'])  # Ensure datetime format\n",
    "df = df.sort_values(by=['ticker', 'publish_date'])\n",
    "\n",
    "# Calculate the average sentiment and add it as a new column\n",
    "df['average_sentiment'] = df[['textblob_sentiment', 'vader_sentiment']].mean(axis=1)\n",
    "\n",
    "# Calculate the daily average sentiment\n",
    "daily_avg_sentiment = df.groupby(df['publish_date'].dt.date)['average_sentiment'].mean().reset_index()\n",
    "daily_avg_sentiment.rename(columns={'average_sentiment': 'daily_average_sentiment', 'publish_date': 'publish_date'}, inplace=True)\n",
    "\n",
    "# Merge the daily average sentiment back into the original DataFrame\n",
    "df['publish_date_date'] = df['publish_date'].dt.date  # Create a column for the date\n",
    "df = df.merge(daily_avg_sentiment, left_on='publish_date_date', right_on='publish_date', how='left')\n",
    "\n",
    "# Remove unnecessary columns introduced during the merge\n",
    "df.drop(columns=['publish_date_y'], inplace=True, errors='ignore')\n",
    "df.rename(columns={'publish_date_x': 'publish_date'}, inplace=True, errors='ignore')\n",
    "\n",
    "# Calculate the daily average market change\n",
    "daily_avg_change = df.groupby(df['publish_date_date'])['Percent_Difference'].mean().reset_index()\n",
    "daily_avg_change.rename(columns={'Percent_Difference': 'Average_Market_Change', 'publish_date_date': 'publish_date_date'}, inplace=True)\n",
    "\n",
    "# Merge the daily average market change back into the original DataFrame\n",
    "df = df.merge(daily_avg_change, left_on='publish_date_date', right_on='publish_date_date', how='left')\n",
    "\n",
    "# Drop the temporary date column\n",
    "df.drop(columns=['publish_date_date'], inplace=True)\n",
    "\n",
    "# Create the new column 'rating_score_change' based on 'percent_diff_7_day_avg_RatingScore'\n",
    "#df['rating_score_change'] = df['RatingScore_3Day_Percent_Diff'].apply(\n",
    "#    lambda x: 1 if x > 0 else (0 if x == 0 else -1)\n",
    "#)\n",
    "\n",
    "# Create the new column 'analyst_score_change' based on 'percent_diff_7_day_avg_analyst_score'\n",
    "#df['analyst_score_change'] = df['analyst_score_3Day_Percent_Diff'].apply(\n",
    "#    lambda x: 1 if x > 0 else (0 if x == 0 else -1)\n",
    "#)\n",
    "\n",
    "# Create the new column 'median_price_change' based on 'percent_diff_7_day_avg_target_median_price'\n",
    "#df['median_price_change'] = df['target_median_price_3Day_Percent_Diff'].apply(\n",
    "#    lambda x: 1 if x > 0 else (0 if x == 0 else -1)\n",
    "#)\n",
    "\n",
    "# Define the columns to include in the new CSV\n",
    "columns_to_include = [\n",
    "    'publish_date',\n",
    "    'ticker',\n",
    "    'publisher',  # Include the publisher column\n",
    "    'average_sentiment',  # Include the new average sentiment column\n",
    "    'daily_average_sentiment',  # Include the daily average sentiment column\n",
    "    'Average_Market_Change',  # Include the new average market change column\n",
    "    'RatingScore_pct_change',  # Include the new rating score change column\n",
    "    'analyst_score_pct_change',  # Include the new analyst score change column\n",
    "    'target_median_price_pct_change',  # Include the new median price change column\n",
    "    'target_score',\n",
    "    'Percent_Difference',\n",
    "    'Forward_15min_Change',\n",
    "    'Forward_30min_Change',\n",
    "    'Forward_45min_Change',\n",
    "    'Forward_60min_Change',\n",
    "]\n",
    "\n",
    "# Filter the DataFrame to include only the specified columns\n",
    "filtered_df = df[columns_to_include]\n",
    "\n",
    "# Drop rows with any empty cells (NaN)\n",
    "filtered_df = filtered_df.dropna()\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "output_file_path = \"Combined_Total_Clean_3.csv\"\n",
    "filtered_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"New CSV file with selected columns and no empty rows saved as {output_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:58: FutureWarning: DataFrameGroupBy.fillna is deprecated and will be removed in a future version. Use obj.ffill() or obj.bfill() for forward or backward filling instead. If you want to fill with a single value, use DataFrame.fillna instead\n",
      "  filtered_df[fill_columns] = filtered_df.groupby('ticker')[fill_columns].fillna(method='ffill')\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:58: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  filtered_df[fill_columns] = filtered_df.groupby('ticker')[fill_columns].fillna(method='ffill')\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:86: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_df[new_column] = filtered_df.groupby('ticker').apply(\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:86: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_df[new_column] = filtered_df.groupby('ticker').apply(\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:86: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_df[new_column] = filtered_df.groupby('ticker').apply(\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:76: FutureWarning: The default fill_method='pad' in Series.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_24388\\2059654833.py:86: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  filtered_df[new_column] = filtered_df.groupby('ticker').apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset with all calculations saved to Combined_Clean_1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"Combined_Raw.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Filter rows where word_count > 7\n",
    "filtered_df = df[df['word_count'] > 7]\n",
    "\n",
    "# Function to check if a string contains only ASCII characters\n",
    "def is_ascii(text):\n",
    "    try:\n",
    "        return text.encode('ascii').decode() == text\n",
    "    except UnicodeEncodeError:\n",
    "        return False\n",
    "\n",
    "# Filter rows where the title column contains only ASCII characters\n",
    "if 'title' in filtered_df.columns:\n",
    "    filtered_df = filtered_df[filtered_df['title'].apply(is_ascii)]\n",
    "\n",
    "# Drop specific columns\n",
    "columns_to_drop = [\n",
    "    'Strong_Buy', 'Buy', 'Hold', 'Sell', 'Strong_Sell',\n",
    "    'hourly_date', 'date_only', 'week_of_year'\n",
    "]\n",
    "filtered_df = filtered_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Ensure the publish_date column is in datetime format\n",
    "filtered_df['publish_date'] = pd.to_datetime(filtered_df['publish_date'], errors='coerce')\n",
    "\n",
    "# Sort the data by ticker and publish_date\n",
    "filtered_df = filtered_df.sort_values(by=['ticker', 'publish_date'])\n",
    "\n",
    "# Define columns to forward fill\n",
    "fill_columns = [\n",
    "    # Original columns\n",
    "    'RatingScore', \n",
    "    'analyst_score', \n",
    "    'reward_score', \n",
    "    'risk_score', \n",
    "    'target_score', \n",
    "    'target_median_price',\n",
    "    # Market data columns\n",
    "    'Close',\n",
    "    'Volume',\n",
    "    'High',\n",
    "    'Low',\n",
    "    'Open'\n",
    "]\n",
    "\n",
    "# Ensure all columns are present in the DataFrame\n",
    "fill_columns = [col for col in fill_columns if col in filtered_df.columns]\n",
    "\n",
    "# Forward fill missing values within each ticker group\n",
    "if fill_columns:\n",
    "    # Group by ticker and forward fill missing values\n",
    "    filtered_df[fill_columns] = filtered_df.groupby('ticker')[fill_columns].fillna(method='ffill')\n",
    "\n",
    "# Calculate Day_Percent_Change for market data\n",
    "filtered_df['Day_Percent_Change'] = ((filtered_df['Close'] - filtered_df['Open']) / filtered_df['Open'] * 100).round(2)\n",
    "\n",
    "# Define columns for daily percent change calculation\n",
    "pct_change_columns = [\n",
    "    'RatingScore',\n",
    "    'analyst_score',\n",
    "    'target_score',\n",
    "    'target_median_price'\n",
    "]\n",
    "\n",
    "# Function to calculate daily percent changes\n",
    "def calculate_daily_pct_change(group, column):\n",
    "    # Create a temporary dataframe with just the first row for each date\n",
    "    daily_values = group.groupby(group['publish_date'].dt.date)[column].first()\n",
    "    # Calculate percent change between days\n",
    "    daily_pct_change = daily_values.pct_change().mul(100).round(2)\n",
    "    # Map these changes back to all rows for each date\n",
    "    return group['publish_date'].dt.date.map(daily_pct_change)\n",
    "\n",
    "# Calculate day-over-day percent changes for specified columns\n",
    "for column in pct_change_columns:\n",
    "    if column in filtered_df.columns:\n",
    "        # Create new column name\n",
    "        new_column = f'{column}_pct_change'\n",
    "        # Calculate percent change within each ticker group\n",
    "        filtered_df[new_column] = filtered_df.groupby('ticker').apply(\n",
    "            lambda x: calculate_daily_pct_change(x, column)\n",
    "        ).reset_index(level=0, drop=True)\n",
    "\n",
    "# Save the resulting dataset\n",
    "output_file_path = \"Combined_Clean_1.csv\"\n",
    "filtered_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Filtered dataset with all calculations saved to {output_file_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching price targets for Ticker ID 146 from Seeking Alpha...\n",
      "Seeking Alpha Response:\n",
      "{\"revisions\":{},\"estimates\":{\"146\":{\"target_price_low\":{\"0\":[{\"effectivedate\":\"2024-12-16T03:53:27.000-05:00\",\"dataitemvalue\":\"184.0\"},{\"effectivedate\":\"2024-12-13T06:15:59.000-05:00\",\"dataitemvalue\":\"184.0\"},{\"effectivedate\":\"2024-12-12T17:16:32.000-05:00\",\"dataitemvalue\":\"184.0\"},{\"effectivedate\":\"2024-12-12T05:35:56.000-05:00\",\"dataitemvalue\":\"184.0\"},{\"effectivedate\":\"2024-12-12T05:16:18.000-05:00\",\"dataitemvalue\":\"184.0\"},{\"effectivedate\":\"2024-12-12T03:13:17.000-05:00\",\"dataitemvalue\":\"184.0\"},{\"effectivedate\":\"2024-11-15T15:41:44.000-05:00\",\"dataitemvalue\":\"184.0\"}]},\"target_price_high\":{\"0\":[{\"effectivedate\":\"2024-12-16T03:53:27.000-05:00\",\"dataitemvalue\":\"300.0\"},{\"effectivedate\":\"2024-12-13T06:15:59.000-05:00\",\"dataitemvalue\":\"300.0\"},{\"effectivedate\":\"2024-12-12T17:16:32.000-05:00\",\"dataitemvalue\":\"300.0\"},{\"effectivedate\":\"2024-12-12T05:35:56.000-05:00\",\"dataitemvalue\":\"300.0\"},{\"effectivedate\":\"2024-12-12T05:16:18.000-05:00\",\"dataitemvalue\":\"300.0\"},{\"effectivedate\":\"2024-12-12T03:13:17.000-05:00\",\"dataitemvalue\":\"300.0\"},{\"effectivedate\":\"2024-11-15T15:41:44.000-05:00\",\"dataitemvalue\":\"300.0\"}]},\"target_price\":{\"0\":[{\"effectivedate\":\"2024-12-16T03:53:27.000-05:00\",\"dataitemvalue\":\"245.78214\"},{\"effectivedate\":\"2024-12-13T06:15:59.000-05:00\",\"dataitemvalue\":\"245.78214\"},{\"effectivedate\":\"2024-12-12T17:16:32.000-05:00\",\"dataitemvalue\":\"245.30595\"},{\"effectivedate\":\"2024-12-12T05:35:56.000-05:00\",\"dataitemvalue\":\"245.30595\"},{\"effectivedate\":\"2024-12-12T05:16:18.000-05:00\",\"dataitemvalue\":\"245.43537\"},{\"effectivedate\":\"2024-12-12T03:13:17.000-05:00\",\"dataitemvalue\":\"245.30595\"},{\"effectivedate\":\"2024-11-15T15:41:44.000-05:00\",\"dataitemvalue\":\"244.47738\"}]}}}}\n",
      "Saving price targets to CSV...\n",
      "Data saved to price_targets.csv\n"
     ]
    }
   ],
   "source": [
    "#Price Targets good.\n",
    "\n",
    "\n",
    "import http.client\n",
    "import json\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "def fetch_seeking_alpha_targets(ticker_id):\n",
    "    print(f\"Fetching price targets for Ticker ID {ticker_id} from Seeking Alpha...\")\n",
    "    conn = http.client.HTTPSConnection(\"seeking-alpha.p.rapidapi.com\")\n",
    "    headers = {\n",
    "        'x-rapidapi-key': \"ee72be2ef9msh532c4fc1a7b7941p1176e1jsn0328598b0245\",\n",
    "        'x-rapidapi-host': \"seeking-alpha.p.rapidapi.com\"\n",
    "    }\n",
    "    conn.request(\"GET\", f\"/symbols/get-analyst-price-target?ticker_ids={ticker_id}&return_window=1&group_by_month=false\", headers=headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "\n",
    "    print(\"Seeking Alpha Response:\")\n",
    "    print(data.decode(\"utf-8\"))  # Debugging: Print raw response\n",
    "\n",
    "    price_targets = {}\n",
    "    try:\n",
    "        parsed_data = json.loads(data.decode(\"utf-8\"))\n",
    "        estimates = parsed_data.get('estimates', {}).get(str(ticker_id), {})\n",
    "        price_targets = {\n",
    "            \"Date\": datetime.now().strftime('%Y-%m-%d'),\n",
    "            \"Target Low\": estimates.get(\"target_price_low\", {}).get(\"0\", [{}])[0].get(\"dataitemvalue\", \"N/A\"),\n",
    "            \"Target Mean\": estimates.get(\"target_price\", {}).get(\"0\", [{}])[0].get(\"dataitemvalue\", \"N/A\"),\n",
    "            \"Target High\": estimates.get(\"target_price_high\", {}).get(\"0\", [{}])[0].get(\"dataitemvalue\", \"N/A\")\n",
    "        }\n",
    "    except (KeyError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error parsing Seeking Alpha price targets: {e}\")\n",
    "    return price_targets\n",
    "\n",
    "def save_price_targets_to_csv(data, filename=\"price_targets.csv\"):\n",
    "    print(\"Saving price targets to CSV...\")\n",
    "    with open(filename, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"Date\", \"Target Low\", \"Target Mean\", \"Target High\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerow(data)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "ticker_id = \"146\"  # Example ticker ID for AAPL\n",
    "price_targets = fetch_seeking_alpha_targets(ticker_id)\n",
    "if price_targets:\n",
    "    save_price_targets_to_csv(price_targets)\n",
    "else:\n",
    "    print(\"No sufficient data available to save.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching recommendations for AAPL from Yahoo Finance...\n",
      "Raw Data:\n",
      " {\"quoteSummary\":{\"result\":[{\"recommendationTrend\":{\"trend\":[{\"period\":\"0m\",\"strongBuy\":8,\"buy\":24,\"hold\":12,\"sell\":1,\"strongSell\":2},{\"period\":\"-1m\",\"strongBuy\":8,\"buy\":24,\"hold\":12,\"sell\":1,\"strongSell\":2},{\"period\":\"-2m\",\"strongBuy\":8,\"buy\":23,\"hold\":12,\"sell\":1,\"strongSell\":2},{\"period\":\"-3m\",\"strongBuy\":8,\"buy\":24,\"hold\":12,\"sell\":0,\"strongSell\":2}],\"maxAge\":86400}}],\"error\":null}}\n",
      "Saving latest recommendations to latest_analyst_recommendations.csv...\n",
      "Data saved to latest_analyst_recommendations.csv\n"
     ]
    }
   ],
   "source": [
    "import http.client\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "def fetch_latest_yahoo_recommendations(ticker):\n",
    "    print(f\"Fetching recommendations for {ticker} from Yahoo Finance...\")\n",
    "    \n",
    "    # Initialize connection\n",
    "    conn = http.client.HTTPSConnection(\"yahoo-finance166.p.rapidapi.com\")\n",
    "    headers = {\n",
    "        'x-rapidapi-key': \"ee72be2ef9msh532c4fc1a7b7941p1176e1jsn0328598b0245\",\n",
    "        'x-rapidapi-host': \"yahoo-finance166.p.rapidapi.com\"\n",
    "    }\n",
    "    \n",
    "    # API request\n",
    "    conn.request(\"GET\", f\"/api/stock/get-recommendation-trend?symbol={ticker}&region=US\", headers=headers)\n",
    "    res = conn.getresponse()\n",
    "    data = res.read()\n",
    "    \n",
    "    # Decode and parse response\n",
    "    decoded_data = data.decode(\"utf-8\")\n",
    "    print(\"Raw Data:\\n\", decoded_data)  # Debugging: Print raw data\n",
    "    parsed_data = json.loads(decoded_data)\n",
    "    \n",
    "    # Extract recommendations\n",
    "    recommendations = parsed_data.get('quoteSummary', {}).get('result', [])[0].get('recommendationTrend', {}).get('trend', [])\n",
    "    \n",
    "    if recommendations:\n",
    "        # Get the latest period's data\n",
    "        latest_trend = recommendations[0]  # First entry is the latest\n",
    "        latest_recommendation = {\n",
    "            \"Ticker\": ticker,\n",
    "            \"Date\": datetime.now().strftime('%Y-%m-%d'),\n",
    "            \"Period\": latest_trend.get('period', 'N/A'),\n",
    "            \"Strong Buy\": latest_trend.get('strongBuy', 0),\n",
    "            \"Buy\": latest_trend.get('buy', 0),\n",
    "            \"Hold\": latest_trend.get('hold', 0),\n",
    "            \"Sell\": latest_trend.get('sell', 0),\n",
    "            \"Strong Sell\": latest_trend.get('strongSell', 0)\n",
    "        }\n",
    "        return latest_recommendation\n",
    "    else:\n",
    "        print(\"No recommendation data found.\")\n",
    "        return None\n",
    "\n",
    "def save_recommendations_to_csv(data, filename=\"latest_analyst_recommendations.csv\"):\n",
    "    print(f\"Saving latest recommendations to {filename}...\")\n",
    "    with open(filename, mode=\"w\", newline=\"\") as file:\n",
    "        fieldnames = [\"Ticker\", \"Date\", \"Period\", \"Strong Buy\", \"Buy\", \"Hold\", \"Sell\", \"Strong Sell\"]\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerow(data)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "ticker = \"AAPL\"\n",
    "latest_recommendation = fetch_latest_yahoo_recommendations(ticker)\n",
    "if latest_recommendation:\n",
    "    save_recommendations_to_csv(latest_recommendation)\n",
    "else:\n",
    "    print(\"No data available to save.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# Google Cloud libraries\n",
    "from google.cloud import bigquery\n",
    "import functions_framework\n",
    "import requests\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize BigQuery client\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Configuration\n",
    "TABLE_ID = os.environ.get('BIGQUERY_TABLE', 'trendsense.stock_data.stock_analyst')\n",
    "YAHOO_API_KEY = \"ee72be2ef9msh532c4fc1a7b7941p1176e1jsn0328598b0245\"\n",
    "SEEKING_ALPHA_API_KEY =\"ee72be2ef9msh532c4fc1a7b7941p1176e1jsn0328598b0245\"\n",
    "\n",
    "# Default tickers if not provided\n",
    "DEFAULT_TICKERS = [\n",
    "    {\"ticker\": \"AAPL\", \"ticker_id\": \"146\"},      # Apple\n",
    "    {\"ticker\": \"GOOGL\", \"ticker_id\": \"97\"},      # Google\n",
    "    {\"ticker\": \"MSFT\", \"ticker_id\": \"152\"},      # Microsoft\n",
    "    {\"ticker\": \"ASTS\", \"ticker_id\": \"12600\"},    # AST SpaceMobile\n",
    "    {\"ticker\": \"PTON\", \"ticker_id\": \"484\"},      # Peloton\n",
    "    {\"ticker\": \"GSAT\", \"ticker_id\": \"821\"},      # Globalstar\n",
    "    {\"ticker\": \"PLTR\", \"ticker_id\": \"347\"},      # Palantir\n",
    "    {\"ticker\": \"SMR\", \"ticker_id\": \"27514\"},     # NuScale Energy\n",
    "    {\"ticker\": \"ACHR\", \"ticker_id\": \"8796\"},     # Archer Aviation\n",
    "    {\"ticker\": \"BWXT\", \"ticker_id\": \"9323\"},     # BWX Technologies\n",
    "    {\"ticker\": \"ARBK\", \"ticker_id\": \"34335\"},    # Arb케k\n",
    "    {\"ticker\": \"AMD\", \"ticker_id\": \"202\"},       # Advanced Micro Devices\n",
    "    {\"ticker\": \"NVDA\", \"ticker_id\": \"164\"},      # NVIDIA\n",
    "    {\"ticker\": \"BTC\", \"ticker_id\": \"57628\"},     # Bitcoin ETF\n",
    "    {\"ticker\": \"GME\", \"ticker_id\": \"364\"},       # GameStop\n",
    "    {\"ticker\": \"MU\", \"ticker_id\": \"122\"},        # Micron Technology\n",
    "    {\"ticker\": \"TSLA\", \"ticker_id\": \"383\"},      # Tesla\n",
    "    {\"ticker\": \"NFLX\", \"ticker_id\": \"220\"},      # Netflix\n",
    "    {\"ticker\": \"ZG\", \"ticker_id\": \"1307\"},       # Zillow\n",
    "    {\"ticker\": \"AVGO\", \"ticker_id\": \"6509\"},     # Broadcom\n",
    "    {\"ticker\": \"SMCI\", \"ticker_id\": \"11001\"},    # Super Micro Computer\n",
    "    {\"ticker\": \"GLW\", \"ticker_id\": \"508\"},       # Corning\n",
    "    {\"ticker\": \"HAL\", \"ticker_id\": \"82\"},        # Halliburton\n",
    "    {\"ticker\": \"LMT\", \"ticker_id\": \"112\"},       # Lockheed Martin\n",
    "    {\"ticker\": \"AMZN\", \"ticker_id\": \"1\"},         # Amazon\n",
    "    {\"ticker\": \"CRM\", \"ticker_id\": \"231\"},       # Salesforce\n",
    "    {\"ticker\": \"NOW\", \"ticker_id\": \"432\"},       # ServiceNow\n",
    "    {\"ticker\": \"CHTR\", \"ticker_id\": \"230\"},      # Charter Communications\n",
    "    {\"ticker\": \"TDS\", \"ticker_id\": \"7958\"},      # Telephone and Data Systems\n",
    "    {\"ticker\": \"META\", \"ticker_id\": \"442\"},      # Meta (Facebook)\n",
    "    {\"ticker\": \"RGTI\", \"ticker_id\": \"35854\"}     # Rigetti Computing\n",
    "]\n",
    "\n",
    "def fetch_latest_yahoo_recommendations(ticker: str) -> Dict[str, Any] | None:\n",
    "    \"\"\"Fetch the latest analyst recommendations from Yahoo Finance.\"\"\"\n",
    "    logger.info(f\"Fetching recommendations for {ticker} from Yahoo Finance...\")\n",
    "    \n",
    "    if not YAHOO_API_KEY:\n",
    "        logger.error(\"Yahoo Finance API key not configured\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        url = f\"https://yahoo-finance166.p.rapidapi.com/api/stock/get-recommendation-trend\"\n",
    "        headers = {\n",
    "            'x-rapidapi-key': YAHOO_API_KEY,\n",
    "            'x-rapidapi-host': 'yahoo-finance166.p.rapidapi.com'\n",
    "        }\n",
    "        params = {\n",
    "            'symbol': ticker,\n",
    "            'region': 'US'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Yahoo API error: {response.status_code} {response.reason}\")\n",
    "            return None\n",
    "        \n",
    "        parsed_data = response.json()\n",
    "        recommendations = parsed_data.get('quoteSummary', {}).get('result', [])[0].get('recommendationTrend', {}).get('trend', [])\n",
    "        \n",
    "        if recommendations:\n",
    "            latest_trend = recommendations[0]\n",
    "            return {\n",
    "                \"Ticker\": ticker,\n",
    "                \"Date\": datetime.now().strftime('%Y-%m-%d'),\n",
    "                \"Period\": latest_trend.get('period', 'N/A'),\n",
    "                \"Strong_Buy\": latest_trend.get('strongBuy', 0),\n",
    "                \"Buy\": latest_trend.get('buy', 0),\n",
    "                \"Hold\": latest_trend.get('hold', 0),\n",
    "                \"Sell\": latest_trend.get('sell', 0),\n",
    "                \"Strong_Sell\": latest_trend.get('strongSell', 0)\n",
    "            }\n",
    "        else:\n",
    "            logger.warning(\"No recommendation data found in the response.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching Yahoo recommendations: {e}\")\n",
    "    return None\n",
    "\n",
    "def fetch_seeking_alpha_targets(ticker_id: str) -> Dict[str, float] | None:\n",
    "    \"\"\"Fetch price target data from Seeking Alpha.\"\"\"\n",
    "    logger.info(f\"Fetching price targets for Ticker ID {ticker_id} from Seeking Alpha...\")\n",
    "    \n",
    "    if not SEEKING_ALPHA_API_KEY:\n",
    "        logger.error(\"Seeking Alpha API key not configured\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        url = \"https://seeking-alpha.p.rapidapi.com/symbols/get-analyst-price-target\"\n",
    "        headers = {\n",
    "            'x-rapidapi-key': SEEKING_ALPHA_API_KEY,\n",
    "            'x-rapidapi-host': 'seeking-alpha.p.rapidapi.com'\n",
    "        }\n",
    "        params = {\n",
    "            'ticker_ids': ticker_id,\n",
    "            'return_window': 1,\n",
    "            'group_by_month': 'false'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Seeking Alpha API error: {response.status_code} {response.reason}\")\n",
    "            return None\n",
    "        \n",
    "        parsed_data = response.json()\n",
    "        estimates = parsed_data.get('estimates', {}).get(str(ticker_id), {})\n",
    "        \n",
    "        return {\n",
    "            \"Target_Low\": float(estimates.get(\"target_price_low\", {}).get(\"0\", [{}])[0].get(\"dataitemvalue\", 0)),\n",
    "            \"Target_Mean\": float(estimates.get(\"target_price\", {}).get(\"0\", [{}])[0].get(\"dataitemvalue\", 0)),\n",
    "            \"Target_High\": float(estimates.get(\"target_price_high\", {}).get(\"0\", [{}])[0].get(\"dataitemvalue\", 0))\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error fetching Seeking Alpha price targets: {e}\")\n",
    "    return None\n",
    "\n",
    "def create_or_get_table() -> bigquery.Table:\n",
    "    \"\"\"\n",
    "    Create a BigQuery table with autodetect schema if it doesn't exist.\n",
    "    \n",
    "    Returns:\n",
    "        The BigQuery table object\n",
    "    \"\"\"\n",
    "    # Parse the table ID\n",
    "    project, dataset, table_name = TABLE_ID.split('.')\n",
    "    \n",
    "    # Construct a full table reference\n",
    "    table_ref = f\"{project}.{dataset}.{table_name}\"\n",
    "    \n",
    "    try:\n",
    "        # Try to get the table\n",
    "        table = client.get_table(table_ref)\n",
    "        logger.info(f\"Table {table_ref} already exists.\")\n",
    "        return table\n",
    "    except Exception:\n",
    "        # Table doesn't exist, so create it\n",
    "        logger.info(f\"Creating table {table_ref} with autodetect schema...\")\n",
    "        \n",
    "        # Define the schema\n",
    "        schema = [\n",
    "            bigquery.SchemaField(\"Ticker\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"Date\", \"DATE\"),\n",
    "            bigquery.SchemaField(\"Period\", \"STRING\"),\n",
    "            bigquery.SchemaField(\"Strong_Buy\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"Buy\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"Hold\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"Sell\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"Strong_Sell\", \"INTEGER\"),\n",
    "            bigquery.SchemaField(\"Target_Low\", \"FLOAT\"),\n",
    "            bigquery.SchemaField(\"Target_Mean\", \"FLOAT\"),\n",
    "            bigquery.SchemaField(\"Target_High\", \"FLOAT\")\n",
    "        ]\n",
    "        \n",
    "        # Create the table\n",
    "        table = bigquery.Table(table_ref, schema=schema)\n",
    "        \n",
    "        # Set table creation options\n",
    "        table.time_partitioning = bigquery.TimePartitioning(\n",
    "            type_=bigquery.TimePartitioningType.DAY,\n",
    "            field=\"Date\"  # Partition by the Date column\n",
    "        )\n",
    "        \n",
    "        # Create the table\n",
    "        table = client.create_table(table)\n",
    "        logger.info(f\"Table {table_ref} created successfully.\")\n",
    "        return table\n",
    "\n",
    "def save_to_bigquery(data: Dict[str, Any]) -> None:\n",
    "    \"\"\"Save combined data to BigQuery.\"\"\"\n",
    "    logger.info(f\"Saving data to BigQuery table {TABLE_ID}...\")\n",
    "    try:\n",
    "        # Ensure the table exists\n",
    "        table = create_or_get_table()\n",
    "        \n",
    "        # Insert the rows\n",
    "        rows_to_insert = [data]\n",
    "        errors = client.insert_rows_json(table, rows_to_insert)\n",
    "        \n",
    "        if not errors:\n",
    "            logger.info(\"Data successfully saved to BigQuery.\")\n",
    "        else:\n",
    "            logger.error(\"Errors occurred while saving to BigQuery: %s\", errors)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving data to BigQuery: {e}\")\n",
    "\n",
    "@functions_framework.http\n",
    "def stock_data_handler(request):\n",
    "    \"\"\"Cloud Function entry point.\"\"\"\n",
    "    logger.info(\"Starting stock data extraction process\")\n",
    "    \n",
    "    try:\n",
    "        # Determine input method\n",
    "        if request.method == 'POST':\n",
    "            # For POST requests, try to get JSON from request body\n",
    "            request_json = request.get_json(silent=True) or {}\n",
    "        else:\n",
    "            # For GET requests or when no JSON is found, use default tickers\n",
    "            request_json = {}\n",
    "        \n",
    "        # Use provided tickers or default\n",
    "        tickers_to_process = request_json.get('tickers', DEFAULT_TICKERS)\n",
    "        \n",
    "        # Process each ticker\n",
    "        results = []\n",
    "        for ticker_info in tickers_to_process:\n",
    "            ticker = ticker_info.get('ticker', 'AAPL').upper()\n",
    "            ticker_id = ticker_info.get('ticker_id', '146')\n",
    "            \n",
    "            yahoo_data = fetch_latest_yahoo_recommendations(ticker)\n",
    "            seeking_alpha_data = fetch_seeking_alpha_targets(ticker_id)\n",
    "            \n",
    "            if yahoo_data and seeking_alpha_data:\n",
    "                combined_data = {**yahoo_data, **seeking_alpha_data}\n",
    "                save_to_bigquery(combined_data)\n",
    "                results.append(f\"Processed {ticker}\")\n",
    "            else:\n",
    "                results.append(f\"Failed to process {ticker}\")\n",
    "        \n",
    "        # Return success response\n",
    "        return (json.dumps({\n",
    "            \"message\": \"Stock data extraction complete\", \n",
    "            \"results\": results\n",
    "        }), 200, {'Content-Type': 'application/json'})\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error in stock_data_handler: {e}\")\n",
    "        return (json.dumps({\n",
    "            \"error\": \"An unexpected error occurred\",\n",
    "            \"details\": str(e)\n",
    "        }), 500, {'Content-Type': 'application/json'})\n",
    "\n",
    "# For local testing\n",
    "if __name__ == \"__main__\":\n",
    "    # You can add local testing logic here if needed\n",
    "    pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yahoo Finance Targets:\n",
      "Recommendations:\n",
      "  period  strongBuy  buy  hold  sell  strongSell\n",
      "0     0m          2    3     0     0           0\n",
      "1    -1m          2    3     0     0           0\n",
      "2    -2m          2    3     0     0           0\n",
      "3    -3m          2    3     0     0           0\n",
      "\n",
      "Price Targets:\n",
      "Current Price: 25.645\n",
      "Target High Price: 53.0\n",
      "Target Low Price: 15.0\n",
      "Target Mean Price: 35.94\n",
      "Target Median Price: 36.0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import yfinance as yf\n",
    "\n",
    "class StockPriceTargetRetriever:\n",
    "    def __init__(self, api_key=None):\n",
    "        \"\"\"\n",
    "        Initialize the Stock Price Target Retriever\n",
    "        \n",
    "        :param api_key: API key for paid services (optional)\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "    \n",
    "    def get_yahoo_finance_target(self, symbol):\n",
    "        \"\"\"\n",
    "        Retrieve price targets and recommendations from Yahoo Finance\n",
    "        \n",
    "        :param symbol: Stock ticker symbol\n",
    "        :return: Dictionary with recommendations and price targets\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Fetch the stock information\n",
    "            stock = yf.Ticker(symbol)\n",
    "            \n",
    "            # Fetch analyst recommendations\n",
    "            recommendations = stock.recommendations\n",
    "            \n",
    "            # Fetch analyst price targets\n",
    "            info = stock.info\n",
    "            \n",
    "            # Extract price target information from stock info\n",
    "            price_targets = {\n",
    "                'current_price': info.get('currentPrice'),\n",
    "                'target_high_price': info.get('targetHighPrice'),\n",
    "                'target_low_price': info.get('targetLowPrice'),\n",
    "                'target_mean_price': info.get('targetMeanPrice'),\n",
    "                'target_median_price': info.get('targetMedianPrice')\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'recommendations': recommendations,\n",
    "                'price_targets': price_targets\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching Yahoo Finance data: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_alpha_vantage_overview(self, symbol):\n",
    "        \"\"\"\n",
    "        Retrieve stock overview from Alpha Vantage\n",
    "        \n",
    "        :param symbol: Stock ticker symbol\n",
    "        :return: Dictionary of stock overview data\n",
    "        \"\"\"\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Alpha Vantage requires an API key\")\n",
    "        \n",
    "        url = f'https://www.alphavantage.co/query?function=OVERVIEW&symbol={symbol}&apikey={self.api_key}'\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching data from Alpha Vantage: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_financial_modeling_prep_target(self, symbol):\n",
    "        \"\"\"\n",
    "        Retrieve price targets from Financial Modeling Prep\n",
    "        \n",
    "        :param symbol: Stock ticker symbol\n",
    "        :return: List of price target data\n",
    "        \"\"\"\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Financial Modeling Prep requires an API key\")\n",
    "        \n",
    "        url = f'https://financialmodelingprep.com/api/v3/price-target?symbol={symbol}&apikey={self.api_key}'\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching data from Financial Modeling Prep: {e}\")\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    # Initialize the retriever\n",
    "    retriever = StockPriceTargetRetriever()\n",
    "    \n",
    "    # Retrieve price targets for Apple (AAPL)\n",
    "    symbol = 'ASTS'\n",
    "    \n",
    "    # Yahoo Finance (completely free)\n",
    "    yahoo_targets = retriever.get_yahoo_finance_target(symbol)\n",
    "    \n",
    "    # Print results with error handling\n",
    "    if yahoo_targets:\n",
    "        print(\"Yahoo Finance Targets:\")\n",
    "        print(\"Recommendations:\")\n",
    "        print(yahoo_targets.get('recommendations', 'No recommendations available'))\n",
    "        print(\"\\nPrice Targets:\")\n",
    "        price_targets = yahoo_targets.get('price_targets', {})\n",
    "        for key, value in price_targets.items():\n",
    "            print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "    else:\n",
    "        print(\"Failed to retrieve stock information.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# Important Notes:\n",
    "# 1. This script requires yfinance library\n",
    "# 2. Install dependencies: pip install yfinance requests\n",
    "# 3. Be aware of potential rate limits or changes in Yahoo Finance's structure\n",
    "\n",
    "# Troubleshooting:\n",
    "# - Ensure you have the latest version of yfinance\n",
    "# - Some stock symbols might not have complete information\n",
    "# - Network connectivity can affect data retrieval\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email sent successfully.\n"
     ]
    }
   ],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "SMTP_SERVER = \"smtp.zoho.com\"\n",
    "SMTP_PORT = 587\n",
    "EMAIL_ADDRESS = \"trendsense@zohomail.com\"\n",
    "EMAIL_PASSWORD = \"pZNUVbUid0tv\"\n",
    "USER_EMAIL_ADDRESS = \"4064315613@vtext.com\"\n",
    "\n",
    "try:\n",
    "    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n",
    "        server.starttls()\n",
    "        server.login(EMAIL_ADDRESS, EMAIL_PASSWORD)\n",
    "        message_body = \"Test email from Cloud Function.\"\n",
    "        msg = MIMEText(message_body)\n",
    "        msg['Subject'] = \"Test Email\"\n",
    "        msg['From'] = EMAIL_ADDRESS\n",
    "        msg['To'] = USER_EMAIL_ADDRESS\n",
    "        server.sendmail(EMAIL_ADDRESS, USER_EMAIL_ADDRESS, msg.as_string())\n",
    "    print(\"Email sent successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error sending email: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "send: 'ehlo DESKTOP-1KT4FIT.home\\r\\n'\n",
      "reply: b'250-mx.zohomail.com Hello DESKTOP-1KT4FIT.home (syn-184-166-077-250.res.spectrum.com (184.166.77.250))\\r\\n'\n",
      "reply: b'250-STARTTLS\\r\\n'\n",
      "reply: b'250 SIZE 32505856\\r\\n'\n",
      "reply: retcode (250); Msg: b'mx.zohomail.com Hello DESKTOP-1KT4FIT.home (syn-184-166-077-250.res.spectrum.com (184.166.77.250))\\nSTARTTLS\\nSIZE 32505856'\n",
      "send: 'STARTTLS\\r\\n'\n",
      "reply: b'220 Ready to start TLS.\\r\\n'\n",
      "reply: retcode (220); Msg: b'Ready to start TLS.'\n",
      "send: 'ehlo DESKTOP-1KT4FIT.home\\r\\n'\n",
      "reply: b'250-mx.zohomail.com Hello DESKTOP-1KT4FIT.home (syn-184-166-077-250.res.spectrum.com (184.166.77.250))\\r\\n'\n",
      "reply: b'250-AUTH LOGIN PLAIN\\r\\n'\n",
      "reply: b'250 SIZE 32505856\\r\\n'\n",
      "reply: retcode (250); Msg: b'mx.zohomail.com Hello DESKTOP-1KT4FIT.home (syn-184-166-077-250.res.spectrum.com (184.166.77.250))\\nAUTH LOGIN PLAIN\\nSIZE 32505856'\n",
      "send: 'AUTH PLAIN AHRyZW5kc2Vuc2VAem9ob21haWwuY29tAHBaTlVWYlVpZDB0dg==\\r\\n'\n",
      "reply: b'235 Authentication Successful\\r\\n'\n",
      "reply: retcode (235); Msg: b'Authentication Successful'\n",
      "send: 'mail FROM:<trendsense@zohomail.com> size=228\\r\\n'\n",
      "reply: b'250 Sender <trendsense@zohomail.com> OK\\r\\n'\n",
      "reply: retcode (250); Msg: b'Sender <trendsense@zohomail.com> OK'\n",
      "send: 'rcpt TO:<trendsense@zohomail.com>\\r\\n'\n",
      "reply: b'250 Recipient <trendsense@zohomail.com> OK\\r\\n'\n",
      "reply: retcode (250); Msg: b'Recipient <trendsense@zohomail.com> OK'\n",
      "send: 'data\\r\\n'\n",
      "reply: b'354 Ok Send data ending with <CRLF>.<CRLF>\\r\\n'\n",
      "reply: retcode (354); Msg: b'Ok Send data ending with <CRLF>.<CRLF>'\n",
      "data: (354, b'Ok Send data ending with <CRLF>.<CRLF>')\n",
      "send: b'Content-Type: text/plain; charset=\"us-ascii\"\\r\\nMIME-Version: 1.0\\r\\nContent-Transfer-Encoding: 7bit\\r\\nSubject: \\r\\nFrom: trendsense@zohomail.com\\r\\nTo: trendsense@zohomail.com\\r\\n\\r\\nTest SMS notification from Zoho sent at 12:31 from vtext.\\r\\n.\\r\\n'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email sent successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reply: b'250 Message received\\r\\n'\n",
      "reply: retcode (250); Msg: b'Message received'\n",
      "data: (250, b'Message received')\n",
      "send: 'QUIT\\r\\n'\n",
      "reply: b'221 mx.zohomail.com closing connection\\r\\n'\n",
      "reply: retcode (221); Msg: b'mx.zohomail.com closing connection'\n"
     ]
    }
   ],
   "source": [
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "SMTP_SERVER = \"smtp.zoho.com\"\n",
    "SMTP_PORT = 587\n",
    "EMAIL_ADDRESS = \"trendsense@zohomail.com\"\n",
    "EMAIL_PASSWORD = \"pZNUVbUid0tv\"\n",
    "VERIZON_SMS_GATEWAY = \"trendsense@zohomail.com\"\n",
    "\n",
    "try:\n",
    "    with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as server:\n",
    "        server.set_debuglevel(1)  # Enable debugging output\n",
    "        server.starttls()\n",
    "        server.login(EMAIL_ADDRESS, EMAIL_PASSWORD)\n",
    "\n",
    "        message_body = \"Test SMS notification from Zoho sent at 12:31 from vtext.\"\n",
    "        msg = MIMEText(message_body)\n",
    "        msg['Subject'] = \"\"  # Optional; may be ignored by SMS gateway\n",
    "        msg['From'] = EMAIL_ADDRESS\n",
    "        msg['To'] = VERIZON_SMS_GATEWAY\n",
    "\n",
    "        server.sendmail(EMAIL_ADDRESS, VERIZON_SMS_GATEWAY, msg.as_string())\n",
    "    print(\"Email sent successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error sending SMS: {e}\")\n",
    "\n",
    " # Email configuration - Replace with your actual credentials\n",
    "    SMTP_SERVER = \"smtp.zoho.com\"\n",
    "    SMTP_PORT = 587\n",
    "    EMAIL_ADDRESS = \"trendsense@zohomail.com\"  # Replace with your email\n",
    "    EMAIL_PASSWORD = \"pZNUVbUid0tv\"  # Replace with your password\n",
    "    USER_EMAIL_ADDRESS = \"trendsense@zohomail.com\"  # Replace with recipient email"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
