{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Yahoo Finance Market News -- Keep\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "def get_market_news(tickers=None, days_back=7):\n",
    "    \"\"\"\n",
    "    Retrieve market news from Yahoo Finance API.\n",
    "    \n",
    "    Parameters:\n",
    "    tickers (list): List of ticker symbols to get news for. If None, gets general market news\n",
    "    days_back (int): Number of days of historical news to retrieve\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: DataFrame containing news items\n",
    "    \"\"\"\n",
    "    # Initialize empty list to store news\n",
    "    all_news = []\n",
    "    \n",
    "    if tickers is None:\n",
    "        # Use ^GSPC (S&P 500) for general market news\n",
    "        tickers = ['^GSPC']\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        # Create ticker object\n",
    "        stock = yf.Ticker(ticker)\n",
    "        \n",
    "        try:\n",
    "            # Get news for the ticker\n",
    "            news = stock.news\n",
    "            \n",
    "            # Process each news item\n",
    "            for item in news:\n",
    "                news_item = {\n",
    "                    'ticker': ticker,\n",
    "                    'title': item.get('title'),\n",
    "                    'publisher': item.get('publisher'),\n",
    "                    'link': item.get('link'),\n",
    "                    'publish_date': datetime.fromtimestamp(item.get('providerPublishTime', 0)),\n",
    "                    'type': item.get('type'),\n",
    "                    'related_tickers': ', '.join(item.get('relatedTickers', []))\n",
    "                }\n",
    "                all_news.append(news_item)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving news for {ticker}: {str(e)}\")\n",
    "    \n",
    "    # Convert to DataFrame and sort by date\n",
    "    news_df = pd.DataFrame(all_news)\n",
    "    if not news_df.empty:\n",
    "        news_df = news_df.sort_values('publish_date', ascending=False)\n",
    "        \n",
    "        # Filter for recent news only\n",
    "        cutoff_date = datetime.now() - timedelta(days=days_back)\n",
    "        news_df = news_df[news_df['publish_date'] >= cutoff_date]\n",
    "    \n",
    "    return news_df\n",
    "\n",
    "def save_news_to_csv(news_df, output_dir='market_news', filename_prefix='market_news'):\n",
    "    \"\"\"\n",
    "    Save news DataFrame to CSV with timestamp in filename.\n",
    "    \n",
    "    Parameters:\n",
    "    news_df (pandas.DataFrame): DataFrame containing news items\n",
    "    output_dir (str): Directory to save CSV files\n",
    "    filename_prefix (str): Prefix for the CSV filename\n",
    "    \n",
    "    Returns:\n",
    "    str: Path to the saved CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "            print(f\"Created directory: {output_dir}\")\n",
    "        \n",
    "        # Generate filename with timestamp\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"{filename_prefix}_{timestamp}.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save to CSV\n",
    "        news_df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "        print(f\"Successfully saved news to: {filepath}\")\n",
    "        \n",
    "        return filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV file: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Create output directory name based on date\n",
    "    today = datetime.now().strftime('%Y%m%d')\n",
    "    output_dir = f\"market_news_{today}\"\n",
    "    \n",
    "    # Get general market news\n",
    "    market_news = get_market_news()\n",
    "    if not market_news.empty:\n",
    "        print(\"\\nGeneral Market News:\")\n",
    "        print(market_news[['publish_date', 'title', 'publisher']].head())\n",
    "        # Save general market news\n",
    "        save_news_to_csv(market_news, output_dir, 'general_market_news')\n",
    "    \n",
    "    # Get news for specific stocks\n",
    "    tech_stocks = ['AAPL', 'GOOGL', 'MSFT']\n",
    "    tech_news = get_market_news(tickers=tech_stocks, days_back=3)\n",
    "    if not tech_news.empty:\n",
    "        print(\"\\nTech Stocks News (Last 3 days):\")\n",
    "        print(tech_news[['ticker', 'publish_date', 'title', 'publisher']].head())\n",
    "        # Save tech stocks news\n",
    "        save_news_to_csv(tech_news, output_dir, 'tech_stocks_news')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 12:51:49,460 - INFO - Starting Russell 2000 news retrieval process\n",
      "2024-11-21 12:51:49,464 - INFO - Fetching news for keywords: none\n",
      "2024-11-21 12:51:49,789 - INFO - Retrieved 50 news items\n",
      "2024-11-21 12:51:49,818 - INFO - Successfully saved news to: c:\\Users\\BryceDaniel\\OneDrive - Lincoln Telephone Company\\MSBA\\GitHub\\TrendSense\\russell2000_news\\russell2000_news_20241121_125149.csv\n",
      "2024-11-21 12:51:49,819 - INFO - \n",
      "Preview of saved data:\n",
      "2024-11-21 12:51:49,829 - INFO -                                                title     publish_date  \\\n",
      "0  TX RX Systems - 50 Years of Leadership in LMR ...  20241121T192100   \n",
      "1  These Analysts Boost Their Forecasts On Wix.co...  20241121T191559   \n",
      "2  How Is The Market Feeling About Equifax? - Equ...  20241121T191518   \n",
      "3  How Is The Market Feeling About Yum Brands? - ...  20241121T191513   \n",
      "4  Williams-Sonoma  ( WSM )  technical analysis -...  20241121T191355   \n",
      "\n",
      "     source  \n",
      "0  Benzinga  \n",
      "1  Benzinga  \n",
      "2  Benzinga  \n",
      "3  Benzinga  \n",
      "4  Benzinga  \n",
      "2024-11-21 12:51:49,839 - INFO - News data has been saved to: c:\\Users\\BryceDaniel\\OneDrive - Lincoln Telephone Company\\MSBA\\GitHub\\TrendSense\\russell2000_news\\russell2000_news_20241121_125149.csv\n",
      "2024-11-21 12:51:49,840 - INFO - Process completed\n"
     ]
    }
   ],
   "source": [
    "###################### AphaVantage Market News - Currently set to Russell 200 so need to change.  Last run gave me day old news\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                   format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_market_news(api_key, keywords=\"none\", limit=10):\n",
    "    \"\"\"\n",
    "    Retrieve market news from Alpha Vantage API.\n",
    "    \n",
    "    Parameters:\n",
    "    api_key (str): Alpha Vantage API key\n",
    "    keywords (str): Search keywords\n",
    "    limit (int): Maximum number of news items to retrieve\n",
    "    \n",
    "    Returns:\n",
    "    list: List of news items\n",
    "    \"\"\"\n",
    "    base_url = 'https://www.alphavantage.co/query'\n",
    "    params = {\n",
    "        'function': 'NEWS_SENTIMENT',\n",
    "        'apikey': api_key,\n",
    "        'keywords': keywords,\n",
    "        'limit': limit\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"Fetching news for keywords: {keywords}\")\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        news_data = response.json()\n",
    "        \n",
    "        if 'Note' in news_data:\n",
    "            logger.warning(f\"API limit message: {news_data['Note']}\")\n",
    "            return None\n",
    "            \n",
    "        if 'feed' in news_data:\n",
    "            logger.info(f\"Retrieved {len(news_data['feed'])} news items\")\n",
    "            return news_data['feed']\n",
    "        else:\n",
    "            logger.warning(\"No news found in response\")\n",
    "            return None\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Request failed: {str(e)}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Failed to parse JSON response: {str(e)}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def save_news_to_csv(news_items, output_dir='russell2000_news'):\n",
    "    \"\"\"\n",
    "    Save news items to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    news_items (list): List of news items\n",
    "    output_dir (str): Directory to save the CSV file\n",
    "    \n",
    "    Returns:\n",
    "    str: Path to the saved CSV file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not news_items:\n",
    "            logger.warning(\"No news items to save\")\n",
    "            return None\n",
    "            \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Process news items with proper string handling\n",
    "        processed_items = []\n",
    "        for item in news_items:\n",
    "            processed_item = {\n",
    "                'title': str(item.get('title', '')),\n",
    "                'publish_date': str(item.get('time_published', '')),\n",
    "                'summary': str(item.get('summary', '')),\n",
    "                'url': str(item.get('url', '')),\n",
    "                'source': str(item.get('source', '')),\n",
    "                'overall_sentiment_score': str(item.get('overall_sentiment_score', '')),\n",
    "                'overall_sentiment_label': str(item.get('overall_sentiment_label', '')),\n",
    "                'topics': ', '.join(str(topic) for topic in item.get('topics', [])),\n",
    "                'ticker_sentiment': json.dumps(item.get('ticker_sentiment', [])),\n",
    "                'authors': ', '.join(str(author) for author in item.get('authors', []))\n",
    "            }\n",
    "            processed_items.append(processed_item)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        news_df = pd.DataFrame(processed_items)\n",
    "        \n",
    "        # Generate filename with timestamp\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"russell2000_news_{timestamp}.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        \n",
    "        # Save to CSV\n",
    "        news_df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "        logger.info(f\"Successfully saved news to: {filepath}\")\n",
    "        \n",
    "        # Print preview of saved data\n",
    "        logger.info(\"\\nPreview of saved data:\")\n",
    "        logger.info(news_df[['title', 'publish_date', 'source']].head())\n",
    "        \n",
    "        return filepath\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving to CSV: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    # Your API key\n",
    "    api_key = 'FLGDYAANWX6EFL9P'\n",
    "    \n",
    "    # Get current directory for saving files\n",
    "    current_dir = os.getcwd()\n",
    "    output_dir = os.path.join(current_dir, 'russell2000_news')\n",
    "    \n",
    "    # Fetch news\n",
    "    news_items = get_market_news(api_key, limit=50)  # Increased limit for more data\n",
    "    \n",
    "    if news_items:\n",
    "        # Save to CSV\n",
    "        saved_file = save_news_to_csv(news_items, output_dir)\n",
    "        if saved_file:\n",
    "            logger.info(f\"News data has been saved to: {saved_file}\")\n",
    "    else:\n",
    "        logger.warning(\"No news data was retrieved\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        logger.info(\"Starting Russell 2000 news retrieval process\")\n",
    "        main()\n",
    "        logger.info(\"Process completed\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Main process error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Income Statement...\n",
      "Fetching Balance Sheet...\n",
      "Fetching Cash Flow...\n",
      "Financial data saved to aapl_10k_financial_data.csv\n"
     ]
    }
   ],
   "source": [
    "############### Alpha Vantage 10K financials.................\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_alpha_vantage_10k_data(api_key, symbol, output_file=\"10k_financial_data.csv\"):\n",
    "    \"\"\"\n",
    "    Fetch key financial data (Income Statement, Balance Sheet, Cash Flow) for a company using Alpha Vantage API.\n",
    "    \n",
    "    Parameters:\n",
    "        api_key (str): Your Alpha Vantage API key.\n",
    "        symbol (str): The stock ticker symbol (e.g., \"AAPL\" for Apple).\n",
    "        output_file (str): Path to save the financial data as a CSV.\n",
    "    \n",
    "    Returns:\n",
    "        None: Saves financial data to a CSV file.\n",
    "    \"\"\"\n",
    "    BASE_URL = \"https://www.alphavantage.co/query\"\n",
    "    endpoints = {\n",
    "        \"income_statement\": \"INCOME_STATEMENT\",\n",
    "        \"balance_sheet\": \"BALANCE_SHEET\",\n",
    "        \"cash_flow\": \"CASH_FLOW\"\n",
    "    }\n",
    "    \n",
    "    financial_data = {}\n",
    "    \n",
    "    for report_type, function in endpoints.items():\n",
    "        print(f\"Fetching {report_type.replace('_', ' ').title()}...\")\n",
    "        \n",
    "        params = {\n",
    "            \"function\": function,\n",
    "            \"symbol\": symbol,\n",
    "            \"apikey\": api_key\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(BASE_URL, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if f\"annualReports\" in data:\n",
    "                # Extract the most recent annual report\n",
    "                financial_data[report_type] = data[\"annualReports\"]\n",
    "            else:\n",
    "                print(f\"Warning: No data found for {report_type}.\")\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {report_type}: {e}\")\n",
    "    \n",
    "    # Combine all financial data into a single DataFrame\n",
    "    combined_data = []\n",
    "    for report_type, reports in financial_data.items():\n",
    "        for report in reports:\n",
    "            report[\"type\"] = report_type\n",
    "            combined_data.append(report)\n",
    "    \n",
    "    # Convert combined data to a DataFrame\n",
    "    if combined_data:\n",
    "        df = pd.DataFrame(combined_data)\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Financial data saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No financial data to save.\")\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    API_KEY = \"FLGDYAANWX6EFL9P\"  # Replace with your Alpha Vantage API key\n",
    "    SYMBOL = \"AAPL\"  # Example: Apple Inc.\n",
    "    get_alpha_vantage_10k_data(API_KEY, SYMBOL, output_file=\"aapl_10k_financial_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving executive narratives or specific sections (like \"Management’s Discussion and Analysis\" or \"Risk Factors\") from 10-K filings requires parsing the full text of the filings. While Alpha Vantage does not support text-based 10-K narrative retrieval, the SEC EDGAR system does. Here's how you can retrieve and parse these sections:\n",
    "\n",
    "Approach to Retrieve Executive Narratives from 10-K\n",
    "1. Using SEC EDGAR API\n",
    "The SEC EDGAR system allows direct access to company filings, including 10-Ks. You can download the full text of a filing in HTML format and parse it to extract specific sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'formType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 80\u001b[0m\n\u001b[0;32m     77\u001b[0m SECTION_TITLE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mManagement’s Discussion and Analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Step 1: Get the filing document URL\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m filing_url \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_10k_filing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCIK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mYEAR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filing_url:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10-K Filing URL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfiling_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[37], line 25\u001b[0m, in \u001b[0;36mfetch_10k_filing\u001b[1;34m(company_cik, filing_type, year)\u001b[0m\n\u001b[0;32m     22\u001b[0m filings \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilings\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecent\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Find the most recent 10-K filing for the specified year\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mfilings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mformType\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformType\u001b[39m\u001b[38;5;124m\"\u001b[39m][i] \u001b[38;5;241m==\u001b[39m filing_type \u001b[38;5;129;01mand\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m filings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilingDate\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]:\n\u001b[0;32m     27\u001b[0m         accession_number \u001b[38;5;241m=\u001b[39m filings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessionNumber\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'formType'"
     ]
    }
   ],
   "source": [
    "################  Analyzing 10K narrative.  Need to fix\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_10k_filing(company_cik, filing_type=\"10-K\", year=\"2023\"):\n",
    "    \"\"\"\n",
    "    Fetch the URL for a company's 10-K filing from SEC EDGAR.\n",
    "\n",
    "    Parameters:\n",
    "        company_cik (str): CIK (Central Index Key) for the company.\n",
    "        filing_type (str): Filing type to search for (default: \"10-K\").\n",
    "        year (str): Filing year (default: \"2023\").\n",
    "\n",
    "    Returns:\n",
    "        str: Filing document URL or None if not found.\n",
    "    \"\"\"\n",
    "    BASE_URL = f\"https://data.sec.gov/submissions/CIK{company_cik}.json\"\n",
    "    headers = {\"User-Agent\": \"YourName email@example.com\"}\n",
    "    \n",
    "    response = requests.get(BASE_URL, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        filings = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "        \n",
    "        # Find the most recent 10-K filing for the specified year\n",
    "        for i in range(len(filings[\"formType\"])):\n",
    "            if filings[\"formType\"][i] == filing_type and year in filings[\"filingDate\"][i]:\n",
    "                accession_number = filings[\"accessionNumber\"][i].replace(\"-\", \"\")\n",
    "                doc_url = f\"https://www.sec.gov/Archives/edgar/data/{company_cik}/{accession_number}/{filings['primaryDocument'][i]}\"\n",
    "                return doc_url\n",
    "    else:\n",
    "        print(f\"Failed to fetch filings for CIK {company_cik}. Status code: {response.status_code}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_section_from_10k(url, section_title):\n",
    "    \"\"\"\n",
    "    Extract a specific section from a 10-K filing document.\n",
    "\n",
    "    Parameters:\n",
    "        url (str): URL to the 10-K filing document.\n",
    "        section_title (str): Title of the section to extract (e.g., \"Management’s Discussion and Analysis\").\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted section text or None if not found.\n",
    "    \"\"\"\n",
    "    headers = {\"User-Agent\": \"YourName email@example.com\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        # Extract text content and find the section\n",
    "        text = soup.get_text(separator=\"\\n\")\n",
    "        lines = text.splitlines()\n",
    "        \n",
    "        # Locate the section by title\n",
    "        section_text = []\n",
    "        capture = False\n",
    "        for line in lines:\n",
    "            if section_title.lower() in line.lower():\n",
    "                capture = True  # Start capturing lines after finding the title\n",
    "            elif capture and line.strip() == \"\":\n",
    "                break  # Stop capturing at the next blank line\n",
    "            if capture:\n",
    "                section_text.append(line)\n",
    "        \n",
    "        return \"\\n\".join(section_text) if section_text else None\n",
    "    else:\n",
    "        print(f\"Failed to fetch 10-K document. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    CIK = \"0000320193\"  # Apple Inc. CIK\n",
    "    YEAR = \"2023\"\n",
    "    SECTION_TITLE = \"Management’s Discussion and Analysis\"\n",
    "    \n",
    "    # Step 1: Get the filing document URL\n",
    "    filing_url = fetch_10k_filing(CIK, year=YEAR)\n",
    "    if filing_url:\n",
    "        print(f\"10-K Filing URL: {filing_url}\")\n",
    "        \n",
    "        # Step 2: Extract the specific section\n",
    "        narrative = extract_section_from_10k(filing_url, SECTION_TITLE)\n",
    "        if narrative:\n",
    "            print(f\"Extracted Section ({SECTION_TITLE}):\\n\")\n",
    "            print(narrative)\n",
    "        else:\n",
    "            print(f\"Section '{SECTION_TITLE}' not found.\")\n",
    "    else:\n",
    "        print(\"10-K filing not found.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
