{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Yahoo Daily News\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from datetime import datetime, timedelta\n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "\n",
    "def analyze_sentiment_vader(text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of a given text using VADER and return both category and score.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        score = analyzer.polarity_scores(text)['compound']\n",
    "        if score > 0.05:\n",
    "            category = \"Positive\"\n",
    "        elif score < -0.05:\n",
    "            category = \"Negative\"\n",
    "        else:\n",
    "            category = \"Neutral\"\n",
    "\n",
    "        return {\"category\": category, \"score\": score}\n",
    "    except Exception as e:\n",
    "        print(f\"Sentiment Analysis Error: {e}\")\n",
    "        return {\"category\": \"Unknown\", \"score\": 0}\n",
    "\n",
    "def get_market_news_with_sentiment(tickers):\n",
    "    \"\"\"\n",
    "    Fetch market news for the current day and analyze sentiment, including sentiment score.\n",
    "    \"\"\"\n",
    "    all_news = []\n",
    "    today = datetime.now().date()\n",
    "    one_day_ago = today - timedelta(days=1)\n",
    "\n",
    "    for ticker in tickers:\n",
    "        stock = yf.Ticker(ticker)\n",
    "\n",
    "        try:\n",
    "            news = stock.news\n",
    "            for item in news:\n",
    "                try:\n",
    "                    publish_timestamp = item.get('providerPublishTime', 0)\n",
    "                    publish_date = datetime.fromtimestamp(publish_timestamp).date()\n",
    "\n",
    "                    # More flexible date filtering\n",
    "                    if publish_date >= one_day_ago:\n",
    "                        sentiment_result = analyze_sentiment_vader(item.get('title', ''))\n",
    "                        news_item = {\n",
    "                            'ticker': ticker,\n",
    "                            'title': item.get('title', ''),\n",
    "                            'publisher': item.get('publisher', ''),\n",
    "                            'link': item.get('link', ''),\n",
    "                            'publish_date': datetime.fromtimestamp(publish_timestamp),\n",
    "                            'sentiment_category': sentiment_result['category'],\n",
    "                            'sentiment_score': sentiment_result['score'],\n",
    "                            'type': item.get('type', ''),\n",
    "                            'related_tickers': ', '.join(item.get('relatedTickers', []))\n",
    "                        }\n",
    "                        all_news.append(news_item)\n",
    "                except Exception as news_item_error:\n",
    "                    print(f\"Error processing news item: {news_item_error}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving news for {ticker}: {str(e)}\")\n",
    "\n",
    "    return pd.DataFrame(all_news)\n",
    "\n",
    "def save_to_bigquery(df, project_id, dataset_id, table_id):\n",
    "    \"\"\"\n",
    "    Save DataFrame to BigQuery using schema auto-detection.\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "    \n",
    "    try:\n",
    "        # Ensure DataFrame is not empty\n",
    "        if df.empty:\n",
    "            print(\"DataFrame is empty. No data to save.\")\n",
    "            return False\n",
    "\n",
    "        # Convert datetime to timestamp\n",
    "        df['publish_date'] = pd.to_datetime(df['publish_date'])\n",
    "\n",
    "        # Configure the job to auto-detect schema and append data\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
    "            autodetect=True  # Enable schema auto-detection\n",
    "        )\n",
    "\n",
    "        # Load DataFrame directly to BigQuery\n",
    "        job = client.load_table_from_dataframe(\n",
    "            df, \n",
    "            table_ref, \n",
    "            job_config=job_config\n",
    "        )\n",
    "\n",
    "        # Wait for the job to complete\n",
    "        job.result()\n",
    "\n",
    "        print(f\"Successfully saved {len(df)} rows to {table_ref}\")\n",
    "        return True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"BigQuery Save Error: {str(e)}\")\n",
    "        print(f\"DataFrame Columns: {df.columns}\")\n",
    "        print(f\"DataFrame Sample:\\n{df.head()}\")\n",
    "        return False\n",
    "\n",
    "def main(request=None):\n",
    "    \"\"\"\n",
    "    Enhanced main function for fetching and saving market news.\n",
    "    \"\"\"\n",
    "    # Google Cloud configuration\n",
    "    project_id = \"trendsense\"\n",
    "    dataset_id = \"market_data\"\n",
    "    table_id = \"market_news_yahoo\"\n",
    "\n",
    "    try:\n",
    "        # Fetch general market news for today\n",
    "        indices = ['^IXIC', '^DJI', '^RUT', '^GSPC']\n",
    "        market_news = get_market_news_with_sentiment(tickers=indices)\n",
    "        if not market_news.empty:\n",
    "            market_news['category'] = 'General'  # Add category for general market\n",
    "\n",
    "        # Fetch tech stock news for today\n",
    "        tech_stocks = ['AAPL', 'GOOGL', 'MSFT', 'ASTS', 'PTON', 'GSAT', 'PLTR', 'SMR', 'ACHR', 'BWXT', 'ARBK', 'AMD', 'NVDA', 'BTC', 'GME', 'MU', 'TSLA', 'NFLX', 'ZG', 'AVGO', 'SMCI','GLW', 'HAL', 'LMT', 'AMZ', 'CRM', 'NOW', 'CHTR', 'TDS', 'META']\n",
    "        tech_news = get_market_news_with_sentiment(tickers=tech_stocks)\n",
    "        if not tech_news.empty:\n",
    "            tech_news['category'] = 'Tech'  # Add category for tech stocks\n",
    "\n",
    "        # Combine news\n",
    "        combined_news = pd.concat([market_news, tech_news], ignore_index=True)\n",
    "\n",
    "        # Save to BigQuery\n",
    "        if not combined_news.empty:\n",
    "            save_result = save_to_bigquery(combined_news, project_id, dataset_id, table_id)\n",
    "            return \"Data successfully saved to BigQuery.\", 200\n",
    "        else:\n",
    "            return \"No news to save.\", 204\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "        return f\"Internal Server Error: {e}\", 500\n",
    "\n",
    "# Optional: For local testing\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General market news fetched: 32 rows\n",
      "Tech stock news fetched: 230 rows\n",
      "Total rows to save: 262\n",
      "Data successfully written to trendsense.market_data.market_news_yahoo_hist.\n"
     ]
    }
   ],
   "source": [
    "## Yahoo History push to Big Query\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from datetime import datetime, timedelta\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "def analyze_sentiment_vader(text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment of a given text using VADER and return both category and score.\n",
    "    \"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    score = analyzer.polarity_scores(text)['compound']\n",
    "    if score > 0.05:\n",
    "        category = \"Positive\"\n",
    "    elif score < -0.05:\n",
    "        category = \"Negative\"\n",
    "    else:\n",
    "        category = \"Neutral\"\n",
    "\n",
    "    return {\"category\": category, \"score\": score}\n",
    "\n",
    "\n",
    "def get_market_news_with_sentiment(tickers, start_date):\n",
    "    \"\"\"\n",
    "    Fetch market news starting from a specific date and analyze sentiment.\n",
    "    \"\"\"\n",
    "    all_news = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        stock = yf.Ticker(ticker)\n",
    "\n",
    "        try:\n",
    "            news = stock.news\n",
    "            for item in news:\n",
    "                publish_date = datetime.fromtimestamp(item.get('providerPublishTime', 0))\n",
    "\n",
    "                # Filter news to include only articles published after the start_date\n",
    "                if publish_date.date() >= start_date:\n",
    "                    sentiment_result = analyze_sentiment_vader(item.get('title'))\n",
    "                    news_item = {\n",
    "                        'ticker': ticker,\n",
    "                        'title': item.get('title'),\n",
    "                        'publisher': item.get('publisher'),\n",
    "                        'link': item.get('link'),\n",
    "                        'publish_date': publish_date,\n",
    "                        'sentiment_category': sentiment_result['category'],\n",
    "                        'sentiment_score': sentiment_result['score'],\n",
    "                        'type': item.get('type'),\n",
    "                        'related_tickers': ', '.join(item.get('relatedTickers', []))\n",
    "                    }\n",
    "                    all_news.append(news_item)\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving news for {ticker}: {str(e)}\")\n",
    "\n",
    "    return pd.DataFrame(all_news)\n",
    "\n",
    "\n",
    "def save_to_bigquery(df, project_id, dataset_id, table_id):\n",
    "    \"\"\"\n",
    "    Save DataFrame to BigQuery, automatically inferring schema and creating the table if necessary.\n",
    "    \"\"\"\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "    \n",
    "    try:\n",
    "        # Check if the table exists\n",
    "        try:\n",
    "            client.get_table(table_ref)\n",
    "        except Exception:\n",
    "            print(f\"Table {table_ref} does not exist. It will be created automatically.\")\n",
    "\n",
    "        # Write DataFrame to BigQuery\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\")\n",
    "        job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "        job.result()  # Wait for the job to complete\n",
    "        print(f\"Data successfully written to {table_ref}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to BigQuery: {str(e)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Entry point for the script.\n",
    "    \"\"\"\n",
    "    # Google Cloud configuration\n",
    "    project_id = \"trendsense\"\n",
    "    dataset_id = \"market_data\"\n",
    "    table_id_hist = \"market_news_yahoo_hist\"  # Historical table\n",
    "\n",
    "    try:\n",
    "        # Define the start date for fetching 3 months of news\n",
    "        start_date = (datetime.now() - timedelta(days=90)).date()\n",
    "\n",
    "        # Fetch general market news for the last 3 months\n",
    "        indices = ['^IXIC', '^DJI', '^RUT', '^GSPC']\n",
    "        market_news = get_market_news_with_sentiment(tickers=indices, start_date=start_date)\n",
    "        print(f\"General market news fetched: {len(market_news)} rows\")\n",
    "        if not market_news.empty:\n",
    "            market_news['category'] = 'General'  # Add category for general market\n",
    "\n",
    "        # Fetch tech stock news for the last 3 months\n",
    "        tech_stocks = ['AAPL', 'GOOGL', 'MSFT', 'ASTS', 'PTON', 'GSAT', 'PLTR', 'SMR', 'ACHR', 'BWXT', 'ARBK', 'AMD', 'NVDA', 'BTC', 'GME', 'MU', 'TSLA', 'NFLX', 'ZG', 'AVGO', 'SMCI', 'GLW', 'HAL', 'LMT', 'AMZ', 'CRM', 'NOW', 'CHTR', 'TDS', 'META']\n",
    "        tech_news = get_market_news_with_sentiment(tickers=tech_stocks, start_date=start_date)\n",
    "        print(f\"Tech stock news fetched: {len(tech_news)} rows\")\n",
    "        if not tech_news.empty:\n",
    "            tech_news['category'] = 'Tech'  # Add category for tech stocks\n",
    "\n",
    "        # Combine news\n",
    "        combined_news = pd.concat([market_news, tech_news], ignore_index=True)\n",
    "        print(f\"Total rows to save: {len(combined_news)}\")\n",
    "\n",
    "        # Save to BigQuery historical table\n",
    "        if not combined_news.empty:\n",
    "            save_to_bigquery(combined_news, project_id, dataset_id, table_id_hist)\n",
    "        else:\n",
    "            print(\"No news to save.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\BryceDaniel/nltk_data', 'c:\\\\Users\\\\BryceDaniel\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data', 'c:\\\\Users\\\\BryceDaniel\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data', 'c:\\\\Users\\\\BryceDaniel\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\BryceDaniel\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "def transfer_market_news_to_hist(request):\n",
    "    \"\"\"\n",
    "    Google Cloud Function to transfer data from market_news_yahoo to market_news_yahoo_hist.\n",
    "\n",
    "    Parameters:\n",
    "        request (flask.Request): The HTTP request object (not used in this implementation).\n",
    "    \"\"\"\n",
    "    project_id = \"trendsense\"\n",
    "    dataset_id = \"market_data\"\n",
    "    source_table_id = \"market_news_yahoo\"\n",
    "    destination_table_id = \"market_news_yahoo_hist\"\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    source_table = f\"{project_id}.{dataset_id}.{source_table_id}\"\n",
    "    destination_table = f\"{project_id}.{dataset_id}.{destination_table_id}\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Query data from source table\n",
    "        print(f\"Querying data from {source_table}...\")\n",
    "        query = f\"SELECT * FROM `{source_table}`\"\n",
    "        source_data = client.query(query).to_dataframe()\n",
    "\n",
    "        if source_data.empty:\n",
    "            print(\"No data found in the source table.\")\n",
    "            return \"No data found.\", 204\n",
    "\n",
    "        print(f\"Fetched {len(source_data)} rows from {source_table}.\")\n",
    "\n",
    "        # Step 2: Insert data into destination table\n",
    "        print(f\"Inserting data into {destination_table}...\")\n",
    "        job = client.load_table_from_dataframe(\n",
    "            source_data,\n",
    "            destination_table,\n",
    "            job_config=bigquery.LoadJobConfig(write_disposition=\"WRITE_APPEND\"),\n",
    "        )\n",
    "        job.result()  # Wait for the job to complete\n",
    "        print(f\"Successfully inserted {len(source_data)} rows into {destination_table}.\")\n",
    "\n",
    "        # Step 3: Clear transferred data from source table\n",
    "        print(\"Deleting transferred data from the source table...\")\n",
    "        delete_query = f\"DELETE FROM `{source_table}` WHERE TRUE\"\n",
    "        client.query(delete_query).result()\n",
    "        print(f\"Cleared data from {source_table}.\")\n",
    "\n",
    "        return \"Data successfully transferred.\", 200\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error transferring data: {str(e)}\")\n",
    "        return f\"Error transferring data: {str(e)}\", 500\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
