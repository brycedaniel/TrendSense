{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline\n",
    "\n",
    "# Replace with your NewsAPI key\n",
    "api_key = 'afc3fe9ac08745439bf521cb5b974fbc'\n",
    "\n",
    "# Initialize sentiment analysis tools\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "bert_sentiment = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# List of tickers to search news for\n",
    "tickers = [\n",
    "    'AAPL', 'GOOGL', 'MSFT', 'ASTS', 'PTON', 'GSAT', 'PLTR', 'SMR', 'ACHR',\n",
    "    'BWXT', 'ARBK', 'AMD', 'NVDA', 'GME', 'MU', 'TSLA', 'NFLX', 'ZG',\n",
    "    'AVGO', 'SMCI', 'GLW', 'HAL', 'LMT', 'AMZN', 'CRM', 'NOW', 'CHTR', 'TDS', 'META'\n",
    "]\n",
    "\n",
    "# Get today's date in ISO format\n",
    "today = datetime.utcnow().strftime('%Y-%m-%d')\n",
    "\n",
    "# Functions for sentiment analysis\n",
    "def vader_sentiment(text):\n",
    "    if text:\n",
    "        return vader_analyzer.polarity_scores(text)['compound']\n",
    "    return 0\n",
    "\n",
    "def textblob_sentiment(text):\n",
    "    if text:\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    return 0\n",
    "\n",
    "def bert_sentiment_analysis(text):\n",
    "    if text:\n",
    "        result = bert_sentiment(text)[0]\n",
    "        return result['label'], result['score']  # Returns sentiment label and confidence\n",
    "    return \"NEUTRAL\", 0.0\n",
    "\n",
    "def bert_to_vader_scale(label, confidence):\n",
    "    label_to_score = {\n",
    "        \"1 star\": -1.0,\n",
    "        \"2 stars\": -0.5,\n",
    "        \"3 stars\": 0.0,\n",
    "        \"4 stars\": 0.5,\n",
    "        \"5 stars\": 1.0\n",
    "    }\n",
    "    return label_to_score.get(label, 0.0) * confidence\n",
    "\n",
    "# Function to fetch market news for the current day\n",
    "def get_market_news(ticker):\n",
    "    url = (\n",
    "        f'https://newsapi.org/v2/everything?q={ticker}&from={today}&to={today}&sortBy=publishedAt&apiKey={api_key}'\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('articles', [])\n",
    "    elif response.status_code == 429:\n",
    "        print(f\"Rate limit exceeded for {ticker}, retrying after delay...\")\n",
    "        time.sleep(5)\n",
    "        return []\n",
    "    else:\n",
    "        print(f\"Error fetching data for {ticker}: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Save data in the required schema\n",
    "def save_to_csv(news_data, filename=\"news_data_today.csv\"):\n",
    "    df = pd.DataFrame(news_data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Fetch and process news for all tickers\n",
    "all_news = []\n",
    "for ticker in tickers:\n",
    "    print(f\"Fetching news for {ticker}...\")\n",
    "    articles = get_market_news(ticker)\n",
    "    \n",
    "    for article in articles:\n",
    "        title = article.get('title', '')\n",
    "        summary = article.get('description', '')\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        headline_vader_sentiment = vader_sentiment(title)\n",
    "        summary_textblob_sentiment = textblob_sentiment(summary)\n",
    "        summary_vader_sentiment = vader_sentiment(summary)\n",
    "        summary_bert_sentiment, bert_confidence = bert_sentiment_analysis(summary)\n",
    "        summary_bert_vader_scaled = bert_to_vader_scale(summary_bert_sentiment, bert_confidence)\n",
    "        \n",
    "        # Article schema\n",
    "        news_entry = {\n",
    "            'ticker': ticker,\n",
    "            'title': title,\n",
    "            'headline_vader_sentiment': headline_vader_sentiment,\n",
    "            'summary': summary,\n",
    "            'summary_textblob_sentiment': summary_textblob_sentiment,\n",
    "            'summary_vader_sentiment': summary_vader_sentiment,\n",
    "            'summary_bert_sentiment': summary_bert_sentiment,\n",
    "            'bert_confidence': bert_confidence,\n",
    "            'summary_bert_vader_scaled': summary_bert_vader_scaled,\n",
    "            'publisher': article.get('source', {}).get('name', ''),\n",
    "            'link': article.get('url', ''),\n",
    "            'publish_date': article.get('publishedAt', ''),\n",
    "            'type': 'general',  # Default value\n",
    "            'related_tickers': '',  # Default empty\n",
    "            'source': 'NewsAPI',  # Identify source\n",
    "        }\n",
    "        all_news.append(news_entry)\n",
    "    \n",
    "    # Avoid rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save the formatted data to a CSV file\n",
    "if all_news:\n",
    "    save_to_csv(all_news)\n",
    "else:\n",
    "    print(\"No news data available.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "class NewsDataProcessorError(Exception):\n",
    "    \"\"\"Custom exception for NewsDataProcessor errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class NewsDataProcessor:\n",
    "    def __init__(self, project_id: str, dataset_id: str, logger: Optional[logging.Logger] = None):\n",
    "        self._validate_input_parameters(project_id, dataset_id)\n",
    "        self.logger = logger or self._setup_logger()\n",
    "        try:\n",
    "            self.client = bigquery.Client(project=project_id)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize BigQuery client: {e}\")\n",
    "            raise NewsDataProcessorError(f\"BigQuery client initialization failed: {e}\")\n",
    "        self.project_id = project_id\n",
    "        self.dataset_id = dataset_id\n",
    "        self.vader_analyzer = SentimentIntensityAnalyzer()\n",
    "        self._bert_pipeline = None\n",
    "\n",
    "    def _validate_input_parameters(self, project_id: str, dataset_id: str):\n",
    "        if not project_id or not isinstance(project_id, str):\n",
    "            raise NewsDataProcessorError(\"Invalid project_id. Must be a non-empty string.\")\n",
    "        if not dataset_id or not isinstance(dataset_id, str):\n",
    "            raise NewsDataProcessorError(\"Invalid dataset_id. Must be a non-empty string.\")\n",
    "\n",
    "    def _setup_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(self.__class__.__name__)\n",
    "        logger.setLevel(logging.DEBUG)  # Set to DEBUG for detailed logs\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.DEBUG)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "        return logger\n",
    "\n",
    "    @property\n",
    "    def bert_pipeline(self):\n",
    "        if self._bert_pipeline is None:\n",
    "            self._bert_pipeline = pipeline(\"sentiment-analysis\")\n",
    "        return self._bert_pipeline\n",
    "\n",
    "    def calculate_vader_sentiment(self, text: Optional[str]) -> float:\n",
    "        if not text or not isinstance(text, str):\n",
    "            return 0.0\n",
    "        try:\n",
    "            sentiment = self.vader_analyzer.polarity_scores(text)\n",
    "            return sentiment.get(\"compound\", 0.0)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"VADER sentiment analysis failed: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def calculate_bert_sentiment(self, text: Optional[str]) -> Tuple[float, float]:\n",
    "        if not text or not isinstance(text, str):\n",
    "            return 0.0, 0.0\n",
    "        try:\n",
    "            result = self.bert_pipeline(text)[0]\n",
    "            # Map BERT sentiment to a range similar to VADER (-1 to 1)\n",
    "            if result[\"label\"] == \"POSITIVE\":\n",
    "                # Scale positive sentiment from 0-1 to 0-1\n",
    "                sentiment_score = (result[\"score\"] * 2) - 1\n",
    "            else:\n",
    "                # Scale negative sentiment from 0-1 to -1-0\n",
    "                sentiment_score = -((result[\"score\"] * 2) - 1)\n",
    "            \n",
    "            confidence = result[\"score\"]\n",
    "            return sentiment_score, confidence\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"BERT sentiment analysis failed: {e}\")\n",
    "            return 0.0, 0.0\n",
    "\n",
    "    def ensure_table_exists(self, table_id: str):\n",
    "        table_ref = f\"{self.project_id}.{self.dataset_id}.{table_id}\"\n",
    "        try:\n",
    "            self.client.get_table(table_ref)\n",
    "            self.logger.info(f\"Table {table_ref} already exists.\")\n",
    "        except Exception:\n",
    "            self.logger.info(f\"Table {table_ref} does not exist. Creating it...\")\n",
    "            schema = [\n",
    "                    bigquery.SchemaField(\"ticker\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"title\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"summary\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"publisher\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"link\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"publish_date\", \"TIMESTAMP\"),  # Use TIMESTAMP for ISO 8601 datetime\n",
    "                    bigquery.SchemaField(\"type\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"related_tickers\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"source\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"lexical_diversity\", \"FLOAT\"),\n",
    "                    bigquery.SchemaField(\"reliability_score\", \"FLOAT\"),\n",
    "                    bigquery.SchemaField(\"textblob_sentiment\", \"FLOAT\"),\n",
    "                    bigquery.SchemaField(\"vader_sentiment\", \"FLOAT\"),\n",
    "                    bigquery.SchemaField(\"bert_sentiment\", \"FLOAT\"),\n",
    "                    bigquery.SchemaField(\"bert_confidence\", \"FLOAT\"),\n",
    "                    bigquery.SchemaField(\"word_count\", \"INTEGER\"),\n",
    "                    bigquery.SchemaField(\"headline_sentiment\", \"FLOAT\"),\n",
    "            ]\n",
    "            table = bigquery.Table(table_ref, schema=schema)\n",
    "            try:\n",
    "                self.client.create_table(table)\n",
    "                self.logger.info(f\"Table {table_ref} created successfully.\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to create table {table_ref}: {e}\")\n",
    "                raise NewsDataProcessorError(f\"Table creation failed: {e}\")\n",
    "    def filter_existing_data(self, new_data: pd.DataFrame, target_table: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filter out rows that already exist in the target table based on publish_date.\n",
    "\n",
    "        Args:\n",
    "            new_data (pd.DataFrame): Incoming new data to be checked for duplicates.\n",
    "            target_table (str): Fully qualified BigQuery table reference (e.g., `trendsense.market_data.Market_News_History_2`).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Filtered dataframe with only new rows.\n",
    "        \"\"\"\n",
    "        if new_data.empty:\n",
    "            self.logger.info(\"No new data provided for filtering.\")\n",
    "            return new_data\n",
    "\n",
    "        try:\n",
    "            # Convert publish_date to ISO string for JSON serialization\n",
    "            new_data['publish_date'] = pd.to_datetime(new_data['publish_date']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "            # Query to fetch existing publish dates\n",
    "            existing_dates_query = f\"\"\"\n",
    "            SELECT DISTINCT FORMAT_TIMESTAMP('%Y-%m-%dT%H:%M:%S', publish_date) AS publish_date\n",
    "            FROM `{target_table}`\n",
    "            WHERE FORMAT_TIMESTAMP('%Y-%m-%dT%H:%M:%S', publish_date) IN UNNEST(@publish_dates)\n",
    "            \"\"\"\n",
    "\n",
    "            # Prepare query parameters\n",
    "            job_config = bigquery.QueryJobConfig(\n",
    "                query_parameters=[\n",
    "                    bigquery.ArrayQueryParameter('publish_dates', 'STRING', new_data['publish_date'].tolist())\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Execute query\n",
    "            query_job = self.client.query(existing_dates_query, job_config=job_config)\n",
    "            existing_dates = [row['publish_date'] for row in query_job]\n",
    "\n",
    "            # Filter out rows with existing publish dates\n",
    "            filtered_data = new_data[~new_data['publish_date'].isin(existing_dates)]\n",
    "            self.logger.info(f\"Total new rows after filtering: {len(filtered_data)} (from {len(new_data)} original rows)\")\n",
    "\n",
    "            return filtered_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error filtering existing data: {e}\")\n",
    "            return new_data\n",
    "\n",
    " \n",
    "\n",
    "    def process_and_move_data(self, source_table_id: str, target_table_id: str, batch_size: int = 1000) -> Dict[str, Any]:\n",
    "        source_table = f\"{self.project_id}.{self.dataset_id}.{source_table_id}\"\n",
    "        target_table = f\"{self.project_id}.{self.dataset_id}.{target_table_id}\"\n",
    "\n",
    "        try:\n",
    "            # Query data from the source table excluding unwanted columns\n",
    "            self.logger.info(f\"Querying source table: {source_table}\")\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                ticker, \n",
    "                title, \n",
    "                summary, \n",
    "                publisher, \n",
    "                link, \n",
    "                publish_date, \n",
    "                type, \n",
    "                related_tickers, \n",
    "                source, \n",
    "                lexical_diversity, \n",
    "                reliability_score, \n",
    "                summary_sentiment\n",
    "            FROM `{source_table}`\n",
    "            LIMIT {batch_size}\n",
    "            \"\"\"\n",
    "            new_data = self.client.query(query).to_dataframe()\n",
    "            self.logger.info(f\"Rows retrieved from source table: {len(new_data)}\")\n",
    "\n",
    "            if new_data.empty:\n",
    "                self.logger.info(\"No new data to process.\")\n",
    "                return {\"status\": \"success\", \"message\": \"No new data\", \"rows_processed\": 0}\n",
    "\n",
    "            # Rename summary_sentiment to textblob_sentiment\n",
    "            self.logger.info(\"Renaming columns...\")\n",
    "            new_data.rename(columns={\"summary_sentiment\": \"textblob_sentiment\"}, inplace=True)\n",
    "\n",
    "            # Ensure publish_date is in datetime format\n",
    "            new_data['publish_date'] = pd.to_datetime(new_data['publish_date'])\n",
    "\n",
    "            # Filter out existing rows\n",
    "            self.logger.info(\"Filtering existing rows...\")\n",
    "            new_data = self.filter_existing_data(new_data, target_table_id)\n",
    "            self.logger.info(f\"Rows remaining after filtering: {len(new_data)}\")\n",
    "\n",
    "            if new_data.empty:\n",
    "                self.logger.info(\"No new unique rows to process after filtering.\")\n",
    "                return {\"status\": \"success\", \"message\": \"No new unique rows\", \"rows_processed\": 0}\n",
    "\n",
    "            # Word Count Calculation\n",
    "            self.logger.info(\"Calculating word count for summaries...\")\n",
    "            new_data[\"word_count\"] = new_data[\"summary\"].fillna(\"\").apply(lambda x: len(str(x).split()))\n",
    "\n",
    "            # Headline Sentiment using VADER\n",
    "            self.logger.info(\"Performing VADER sentiment analysis on headlines...\")\n",
    "            new_data[\"headline_sentiment\"] = new_data[\"title\"].apply(self.calculate_vader_sentiment)\n",
    "\n",
    "            # Existing Sentiment Analyses\n",
    "            self.logger.info(\"Performing VADER sentiment analysis on summaries...\")\n",
    "            new_data[\"vader_sentiment\"] = new_data[\"summary\"].apply(self.calculate_vader_sentiment)\n",
    "            bert_results = new_data[\"summary\"].apply(self.calculate_bert_sentiment).tolist()\n",
    "\n",
    "            # Validate BERT results\n",
    "            if len(bert_results) != len(new_data):\n",
    "                self.logger.error(f\"BERT results length mismatch: {len(bert_results)} results for {len(new_data)} rows.\")\n",
    "                raise ValueError(\"BERT results length mismatch with DataFrame rows.\")\n",
    "\n",
    "            # Unpack BERT results into separate columns\n",
    "            bert_sentiments, bert_confidences = zip(*bert_results)\n",
    "            new_data[\"bert_sentiment\"] = bert_sentiments\n",
    "            new_data[\"bert_confidence\"] = bert_confidences\n",
    "\n",
    "            # Load data into the target table\n",
    "            self.logger.info(\"Loading data into BigQuery...\")\n",
    "            job_config = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_APPEND)\n",
    "            job = self.client.load_table_from_dataframe(new_data, target_table, job_config=job_config)\n",
    "            job.result()  # Wait for the job to complete\n",
    "\n",
    "            success_msg = f\"Data successfully moved to {target_table}. Rows added: {len(new_data)}\"\n",
    "            self.logger.info(success_msg)\n",
    "\n",
    "            return {\"status\": \"success\", \"message\": success_msg, \"rows_processed\": len(new_data)}\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing data: {e}\"\n",
    "            self.logger.error(error_msg)\n",
    "            return {\"status\": \"error\", \"message\": error_msg, \"rows_processed\": 0}\n",
    "\n",
    "\n",
    "def move_market_news_data(request):\n",
    "    \"\"\"\n",
    "    Google Cloud Function entry point to process and move market news data.\n",
    "    \"\"\"\n",
    "    # Load configuration from environment variables\n",
    "    project_id = os.getenv('GCP_PROJECT_ID', 'trendsense')\n",
    "    dataset_id = os.getenv('BQ_DATASET_ID', 'market_data')\n",
    "    source_table_id = os.getenv('SOURCE_TABLE_ID', 'Market_News_History_New')\n",
    "    target_table_id = os.getenv('TARGET_TABLE_ID', 'Market_News_History_2')\n",
    "\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    try:\n",
    "        # Initialize the processor and ensure the target table exists\n",
    "        processor = NewsDataProcessor(project_id, dataset_id)\n",
    "        processor.ensure_table_exists(target_table_id)\n",
    "\n",
    "        # Process and move data\n",
    "        result = processor.process_and_move_data(source_table_id, target_table_id)\n",
    "\n",
    "        # Return the result in a response\n",
    "        return {\n",
    "            'statusCode': 200 if result['status'] == 'success' else 500,\n",
    "            'body': result\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process market news data: {e}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': {\n",
    "                'status': 'error',\n",
    "                'message': str(e)\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bryce\\AppData\\Local\\Temp\\ipykernel_29372\\4142841593.py:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  end_date = datetime.utcnow()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for AAPL from 2024-11-30 to 2024-12-10...\n",
      "Data successfully saved to AAPL_news_2024-11-30_to_2024-12-10.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Replace with your NewsAPI key\n",
    "api_key = 'afc3fe9ac08745439bf521cb5b974fbc'\n",
    "\n",
    "# Specify the ticker you want to test\n",
    "ticker = 'AAPL'\n",
    "\n",
    "# Get the date 10 days ago and today in ISO format\n",
    "end_date = datetime.utcnow()\n",
    "start_date = end_date - timedelta(days=10)\n",
    "start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "# Function for TextBlob sentiment analysis\n",
    "def textblob_sentiment(text):\n",
    "    if text:\n",
    "        return TextBlob(text).sentiment.polarity  # Sentiment polarity from -1 to 1\n",
    "    return 0\n",
    "\n",
    "# Function to fetch market news for a date range\n",
    "def get_market_news(ticker, start_date, end_date):\n",
    "    url = (\n",
    "        f'https://newsapi.org/v2/everything?q={ticker}&from={start_date}&to={end_date}&sortBy=publishedAt&apiKey={api_key}'\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('articles', [])\n",
    "    elif response.status_code == 429:\n",
    "        print(f\"Rate limit exceeded for {ticker}, retrying after delay...\")\n",
    "        time.sleep(5)\n",
    "        return []\n",
    "    else:\n",
    "        print(f\"Error fetching data for {ticker}: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Main function to fetch news and save to CSV\n",
    "def main():\n",
    "    all_news = []\n",
    "    print(f\"Fetching news for {ticker} from {start_date_str} to {end_date_str}...\")\n",
    "    articles = get_market_news(ticker, start_date_str, end_date_str)\n",
    "\n",
    "    for article in articles:\n",
    "        title = article.get('title', '')\n",
    "        summary = article.get('description', '')\n",
    "\n",
    "        # Sentiment analysis using TextBlob\n",
    "        summary_textblob_sentiment = textblob_sentiment(summary)\n",
    "\n",
    "        # Article schema\n",
    "        news_entry = {\n",
    "            'ticker': ticker,\n",
    "            'title': title,\n",
    "            'summary': summary,\n",
    "            'summary_textblob_sentiment': summary_textblob_sentiment,\n",
    "            'publisher': article.get('source', {}).get('name', ''),\n",
    "            'link': article.get('url', ''),\n",
    "            'publish_date': article.get('publishedAt', ''),\n",
    "            'source': 'NewsAPI',  # Identify source\n",
    "        }\n",
    "        all_news.append(news_entry)\n",
    "\n",
    "    # Convert data to a DataFrame\n",
    "    df = pd.DataFrame(all_news)\n",
    "\n",
    "    if not df.empty:\n",
    "        # Save to CSV\n",
    "        output_file = f\"{ticker}_news_{start_date_str}_to_{end_date_str}.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Data successfully saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No news articles found for the specified date range.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
