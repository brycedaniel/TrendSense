{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline\n",
    "\n",
    "# Replace with your NewsAPI key\n",
    "api_key = 'afc3fe9ac08745439bf521cb5b974fbc'\n",
    "\n",
    "# Initialize sentiment analysis tools\n",
    "vader_analyzer = SentimentIntensityAnalyzer()\n",
    "bert_sentiment = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# List of tickers to search news for\n",
    "tickers = [\n",
    "    'AAPL', 'GOOGL', 'MSFT', 'ASTS', 'PTON', 'GSAT', 'PLTR', 'SMR', 'ACHR',\n",
    "    'BWXT', 'ARBK', 'AMD', 'NVDA', 'GME', 'MU', 'TSLA', 'NFLX', 'ZG',\n",
    "    'AVGO', 'SMCI', 'GLW', 'HAL', 'LMT', 'AMZN', 'CRM', 'NOW', 'CHTR', 'TDS', 'META'\n",
    "]\n",
    "\n",
    "# Get today's date in ISO format\n",
    "today = datetime.utcnow().strftime('%Y-%m-%d')\n",
    "\n",
    "# Functions for sentiment analysis\n",
    "def vader_sentiment(text):\n",
    "    if text:\n",
    "        return vader_analyzer.polarity_scores(text)['compound']\n",
    "    return 0\n",
    "\n",
    "def textblob_sentiment(text):\n",
    "    if text:\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    return 0\n",
    "\n",
    "def bert_sentiment_analysis(text):\n",
    "    if text:\n",
    "        result = bert_sentiment(text)[0]\n",
    "        return result['label'], result['score']  # Returns sentiment label and confidence\n",
    "    return \"NEUTRAL\", 0.0\n",
    "\n",
    "def bert_to_vader_scale(label, confidence):\n",
    "    label_to_score = {\n",
    "        \"1 star\": -1.0,\n",
    "        \"2 stars\": -0.5,\n",
    "        \"3 stars\": 0.0,\n",
    "        \"4 stars\": 0.5,\n",
    "        \"5 stars\": 1.0\n",
    "    }\n",
    "    return label_to_score.get(label, 0.0) * confidence\n",
    "\n",
    "# Function to fetch market news for the current day\n",
    "def get_market_news(ticker):\n",
    "    url = (\n",
    "        f'https://newsapi.org/v2/everything?q={ticker}&from={today}&to={today}&sortBy=publishedAt&apiKey={api_key}'\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('articles', [])\n",
    "    elif response.status_code == 429:\n",
    "        print(f\"Rate limit exceeded for {ticker}, retrying after delay...\")\n",
    "        time.sleep(5)\n",
    "        return []\n",
    "    else:\n",
    "        print(f\"Error fetching data for {ticker}: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Save data in the required schema\n",
    "def save_to_csv(news_data, filename=\"news_data_today.csv\"):\n",
    "    df = pd.DataFrame(news_data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "# Fetch and process news for all tickers\n",
    "all_news = []\n",
    "for ticker in tickers:\n",
    "    print(f\"Fetching news for {ticker}...\")\n",
    "    articles = get_market_news(ticker)\n",
    "    \n",
    "    for article in articles:\n",
    "        title = article.get('title', '')\n",
    "        summary = article.get('description', '')\n",
    "        \n",
    "        # Sentiment analysis\n",
    "        headline_vader_sentiment = vader_sentiment(title)\n",
    "        summary_textblob_sentiment = textblob_sentiment(summary)\n",
    "        summary_vader_sentiment = vader_sentiment(summary)\n",
    "        summary_bert_sentiment, bert_confidence = bert_sentiment_analysis(summary)\n",
    "        summary_bert_vader_scaled = bert_to_vader_scale(summary_bert_sentiment, bert_confidence)\n",
    "        \n",
    "        # Article schema\n",
    "        news_entry = {\n",
    "            'ticker': ticker,\n",
    "            'title': title,\n",
    "            'headline_vader_sentiment': headline_vader_sentiment,\n",
    "            'summary': summary,\n",
    "            'summary_textblob_sentiment': summary_textblob_sentiment,\n",
    "            'summary_vader_sentiment': summary_vader_sentiment,\n",
    "            'summary_bert_sentiment': summary_bert_sentiment,\n",
    "            'bert_confidence': bert_confidence,\n",
    "            'summary_bert_vader_scaled': summary_bert_vader_scaled,\n",
    "            'publisher': article.get('source', {}).get('name', ''),\n",
    "            'link': article.get('url', ''),\n",
    "            'publish_date': article.get('publishedAt', ''),\n",
    "            'type': 'general',  # Default value\n",
    "            'related_tickers': '',  # Default empty\n",
    "            'source': 'NewsAPI',  # Identify source\n",
    "        }\n",
    "        all_news.append(news_entry)\n",
    "    \n",
    "    # Avoid rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save the formatted data to a CSV file\n",
    "if all_news:\n",
    "    save_to_csv(all_news)\n",
    "else:\n",
    "    print(\"No news data available.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from google.cloud import bigquery\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "class NewsDataProcessorError(Exception):\n",
    "    \"\"\"Custom exception for NewsDataProcessor errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class NewsDataProcessor:\n",
    "    def __init__(self, project_id: str, dataset_id: str, logger: Optional[logging.Logger] = None):\n",
    "        self._validate_input_parameters(project_id, dataset_id)\n",
    "        self.logger = logger or self._setup_logger()\n",
    "        try:\n",
    "            self.client = bigquery.Client(project=project_id)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize BigQuery client: {e}\")\n",
    "            raise NewsDataProcessorError(f\"BigQuery client initialization failed: {e}\")\n",
    "        self.project_id = project_id\n",
    "        self.dataset_id = dataset_id\n",
    "        self.vader_analyzer = SentimentIntensityAnalyzer()\n",
    "        self._bert_pipeline = None\n",
    "\n",
    "    def _validate_input_parameters(self, project_id: str, dataset_id: str):\n",
    "        if not project_id or not isinstance(project_id, str):\n",
    "            raise NewsDataProcessorError(\"Invalid project_id. Must be a non-empty string.\")\n",
    "        if not dataset_id or not isinstance(dataset_id, str):\n",
    "            raise NewsDataProcessorError(\"Invalid dataset_id. Must be a non-empty string.\")\n",
    "\n",
    "    def _setup_logger(self) -> logging.Logger:\n",
    "        logger = logging.getLogger(self.__class__.__name__)\n",
    "        logger.setLevel(logging.DEBUG)  # Set to DEBUG for detailed logs\n",
    "        console_handler = logging.StreamHandler()\n",
    "        console_handler.setLevel(logging.DEBUG)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        console_handler.setFormatter(formatter)\n",
    "        logger.addHandler(console_handler)\n",
    "        return logger\n",
    "\n",
    "    @property\n",
    "    def bert_pipeline(self):\n",
    "        if self._bert_pipeline is None:\n",
    "            self._bert_pipeline = pipeline(\"sentiment-analysis\")\n",
    "        return self._bert_pipeline\n",
    "\n",
    "    def calculate_vader_sentiment(self, text: Optional[str]) -> float:\n",
    "        if not text or not isinstance(text, str):\n",
    "            return 0.0\n",
    "        try:\n",
    "            sentiment = self.vader_analyzer.polarity_scores(text)\n",
    "            return sentiment.get(\"compound\", 0.0)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"VADER sentiment analysis failed: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def calculate_bert_sentiment(self, text: Optional[str]) -> Tuple[float, float]:\n",
    "        if not text or not isinstance(text, str):\n",
    "            return 0.0, 0.0\n",
    "        try:\n",
    "            result = self.bert_pipeline(text)[0]\n",
    "            # Map BERT sentiment to a range similar to VADER (-1 to 1)\n",
    "            if result[\"label\"] == \"POSITIVE\":\n",
    "                # Scale positive sentiment from 0-1 to 0-1\n",
    "                sentiment_score = (result[\"score\"] * 2) - 1\n",
    "            else:\n",
    "                # Scale negative sentiment from 0-1 to -1-0\n",
    "                sentiment_score = -((result[\"score\"] * 2) - 1)\n",
    "            \n",
    "            confidence = result[\"score\"]\n",
    "            return sentiment_score, confidence\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"BERT sentiment analysis failed: {e}\")\n",
    "            return 0.0, 0.0\n",
    "\n",
    "    def ensure_table_exists(self, table_id: str):\n",
    "        table_ref = f\"{self.project_id}.{self.dataset_id}.{table_id}\"\n",
    "        try:\n",
    "            self.client.get_table(table_ref)\n",
    "            self.logger.info(f\"Table {table_ref} already exists.\")\n",
    "        except Exception:\n",
    "            self.logger.info(f\"Table {table_ref} does not exist. Creating it...\")\n",
    "            schema = [\n",
    "                    bigquery.SchemaField(\"ticker\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"title\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"summary\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"publisher\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"link\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"publish_date\", \"TIMESTAMP\"),  # Use TIMESTAMP for ISO 8601 datetime\n",
    "                    bigquery.SchemaField(\"type\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"related_tickers\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"source\", \"STRING\"),\n",
    "                    bigquery.SchemaField(\"lexical_diversity\", \"FLOAT\"),\n",
    "                    bigquery.SchemaField(\"reliability_score\", \"FLOAT\"),\n",
    "                    bigquery.SchemaField(\"textblob_sentiment\", \"FLOAT\"),\n",
    "                    bigquery.SchemaField(\"vader_sentiment\", \"FLOAT\"),\n",
    "                    bigquery.SchemaField(\"bert_sentiment\", \"FLOAT\"),\n",
    "                    bigquery.SchemaField(\"bert_confidence\", \"FLOAT\"),\n",
    "                    bigquery.SchemaField(\"word_count\", \"INTEGER\"),\n",
    "                    bigquery.SchemaField(\"headline_sentiment\", \"FLOAT\"),\n",
    "            ]\n",
    "            table = bigquery.Table(table_ref, schema=schema)\n",
    "            try:\n",
    "                self.client.create_table(table)\n",
    "                self.logger.info(f\"Table {table_ref} created successfully.\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to create table {table_ref}: {e}\")\n",
    "                raise NewsDataProcessorError(f\"Table creation failed: {e}\")\n",
    "    def filter_existing_data(self, new_data: pd.DataFrame, target_table: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filter out rows that already exist in the target table based on publish_date.\n",
    "\n",
    "        Args:\n",
    "            new_data (pd.DataFrame): Incoming new data to be checked for duplicates.\n",
    "            target_table (str): Fully qualified BigQuery table reference (e.g., `trendsense.market_data.Market_News_History_2`).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Filtered dataframe with only new rows.\n",
    "        \"\"\"\n",
    "        if new_data.empty:\n",
    "            self.logger.info(\"No new data provided for filtering.\")\n",
    "            return new_data\n",
    "\n",
    "        try:\n",
    "            # Convert publish_date to ISO string for JSON serialization\n",
    "            new_data['publish_date'] = pd.to_datetime(new_data['publish_date']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "            # Query to fetch existing publish dates\n",
    "            existing_dates_query = f\"\"\"\n",
    "            SELECT DISTINCT FORMAT_TIMESTAMP('%Y-%m-%dT%H:%M:%S', publish_date) AS publish_date\n",
    "            FROM `{target_table}`\n",
    "            WHERE FORMAT_TIMESTAMP('%Y-%m-%dT%H:%M:%S', publish_date) IN UNNEST(@publish_dates)\n",
    "            \"\"\"\n",
    "\n",
    "            # Prepare query parameters\n",
    "            job_config = bigquery.QueryJobConfig(\n",
    "                query_parameters=[\n",
    "                    bigquery.ArrayQueryParameter('publish_dates', 'STRING', new_data['publish_date'].tolist())\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Execute query\n",
    "            query_job = self.client.query(existing_dates_query, job_config=job_config)\n",
    "            existing_dates = [row['publish_date'] for row in query_job]\n",
    "\n",
    "            # Filter out rows with existing publish dates\n",
    "            filtered_data = new_data[~new_data['publish_date'].isin(existing_dates)]\n",
    "            self.logger.info(f\"Total new rows after filtering: {len(filtered_data)} (from {len(new_data)} original rows)\")\n",
    "\n",
    "            return filtered_data\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error filtering existing data: {e}\")\n",
    "            return new_data\n",
    "\n",
    " \n",
    "\n",
    "    def process_and_move_data(self, source_table_id: str, target_table_id: str, batch_size: int = 1000) -> Dict[str, Any]:\n",
    "        source_table = f\"{self.project_id}.{self.dataset_id}.{source_table_id}\"\n",
    "        target_table = f\"{self.project_id}.{self.dataset_id}.{target_table_id}\"\n",
    "\n",
    "        try:\n",
    "            # Query data from the source table excluding unwanted columns\n",
    "            self.logger.info(f\"Querying source table: {source_table}\")\n",
    "            query = f\"\"\"\n",
    "            SELECT \n",
    "                ticker, \n",
    "                title, \n",
    "                summary, \n",
    "                publisher, \n",
    "                link, \n",
    "                publish_date, \n",
    "                type, \n",
    "                related_tickers, \n",
    "                source, \n",
    "                lexical_diversity, \n",
    "                reliability_score, \n",
    "                summary_sentiment\n",
    "            FROM `{source_table}`\n",
    "            LIMIT {batch_size}\n",
    "            \"\"\"\n",
    "            new_data = self.client.query(query).to_dataframe()\n",
    "            self.logger.info(f\"Rows retrieved from source table: {len(new_data)}\")\n",
    "\n",
    "            if new_data.empty:\n",
    "                self.logger.info(\"No new data to process.\")\n",
    "                return {\"status\": \"success\", \"message\": \"No new data\", \"rows_processed\": 0}\n",
    "\n",
    "            # Rename summary_sentiment to textblob_sentiment\n",
    "            self.logger.info(\"Renaming columns...\")\n",
    "            new_data.rename(columns={\"summary_sentiment\": \"textblob_sentiment\"}, inplace=True)\n",
    "\n",
    "            # Ensure publish_date is in datetime format\n",
    "            new_data['publish_date'] = pd.to_datetime(new_data['publish_date'])\n",
    "\n",
    "            # Filter out existing rows\n",
    "            self.logger.info(\"Filtering existing rows...\")\n",
    "            new_data = self.filter_existing_data(new_data, target_table_id)\n",
    "            self.logger.info(f\"Rows remaining after filtering: {len(new_data)}\")\n",
    "\n",
    "            if new_data.empty:\n",
    "                self.logger.info(\"No new unique rows to process after filtering.\")\n",
    "                return {\"status\": \"success\", \"message\": \"No new unique rows\", \"rows_processed\": 0}\n",
    "\n",
    "            # Word Count Calculation\n",
    "            self.logger.info(\"Calculating word count for summaries...\")\n",
    "            new_data[\"word_count\"] = new_data[\"summary\"].fillna(\"\").apply(lambda x: len(str(x).split()))\n",
    "\n",
    "            # Headline Sentiment using VADER\n",
    "            self.logger.info(\"Performing VADER sentiment analysis on headlines...\")\n",
    "            new_data[\"headline_sentiment\"] = new_data[\"title\"].apply(self.calculate_vader_sentiment)\n",
    "\n",
    "            # Existing Sentiment Analyses\n",
    "            self.logger.info(\"Performing VADER sentiment analysis on summaries...\")\n",
    "            new_data[\"vader_sentiment\"] = new_data[\"summary\"].apply(self.calculate_vader_sentiment)\n",
    "            bert_results = new_data[\"summary\"].apply(self.calculate_bert_sentiment).tolist()\n",
    "\n",
    "            # Validate BERT results\n",
    "            if len(bert_results) != len(new_data):\n",
    "                self.logger.error(f\"BERT results length mismatch: {len(bert_results)} results for {len(new_data)} rows.\")\n",
    "                raise ValueError(\"BERT results length mismatch with DataFrame rows.\")\n",
    "\n",
    "            # Unpack BERT results into separate columns\n",
    "            bert_sentiments, bert_confidences = zip(*bert_results)\n",
    "            new_data[\"bert_sentiment\"] = bert_sentiments\n",
    "            new_data[\"bert_confidence\"] = bert_confidences\n",
    "\n",
    "            # Load data into the target table\n",
    "            self.logger.info(\"Loading data into BigQuery...\")\n",
    "            job_config = bigquery.LoadJobConfig(write_disposition=bigquery.WriteDisposition.WRITE_APPEND)\n",
    "            job = self.client.load_table_from_dataframe(new_data, target_table, job_config=job_config)\n",
    "            job.result()  # Wait for the job to complete\n",
    "\n",
    "            success_msg = f\"Data successfully moved to {target_table}. Rows added: {len(new_data)}\"\n",
    "            self.logger.info(success_msg)\n",
    "\n",
    "            return {\"status\": \"success\", \"message\": success_msg, \"rows_processed\": len(new_data)}\n",
    "\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing data: {e}\"\n",
    "            self.logger.error(error_msg)\n",
    "            return {\"status\": \"error\", \"message\": error_msg, \"rows_processed\": 0}\n",
    "\n",
    "\n",
    "def move_market_news_data(request):\n",
    "    \"\"\"\n",
    "    Google Cloud Function entry point to process and move market news data.\n",
    "    \"\"\"\n",
    "    # Load configuration from environment variables\n",
    "    project_id = os.getenv('GCP_PROJECT_ID', 'trendsense')\n",
    "    dataset_id = os.getenv('BQ_DATASET_ID', 'market_data')\n",
    "    source_table_id = os.getenv('SOURCE_TABLE_ID', 'Market_News_History_New')\n",
    "    target_table_id = os.getenv('TARGET_TABLE_ID', 'Market_News_History_2')\n",
    "\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    try:\n",
    "        # Initialize the processor and ensure the target table exists\n",
    "        processor = NewsDataProcessor(project_id, dataset_id)\n",
    "        processor.ensure_table_exists(target_table_id)\n",
    "\n",
    "        # Process and move data\n",
    "        result = processor.process_and_move_data(source_table_id, target_table_id)\n",
    "\n",
    "        # Return the result in a response\n",
    "        return {\n",
    "            'statusCode': 200 if result['status'] == 'success' else 500,\n",
    "            'body': result\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process market news data: {e}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': {\n",
    "                'status': 'error',\n",
    "                'message': str(e)\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching rating for AAPL...\n",
      "Fetching rating for GOOGL...\n",
      "Fetching rating for MSFT...\n",
      "Fetching rating for ASTS...\n",
      "Fetching rating for PTON...\n",
      "Fetching rating for GSAT...\n",
      "Fetching rating for PLTR...\n",
      "Fetching rating for SMR...\n",
      "Fetching rating for ACHR...\n",
      "Fetching rating for BWXT...\n",
      "Fetching rating for ARBK...\n",
      "Fetching rating for AMD...\n",
      "Fetching rating for NVDA...\n",
      "Fetching rating for GME...\n",
      "Fetching rating for MU...\n",
      "Fetching rating for TSLA...\n",
      "Fetching rating for NFLX...\n",
      "Fetching rating for ZG...\n",
      "Fetching rating for AVGO...\n",
      "Fetching rating for SMCI...\n",
      "Fetching rating for GLW...\n",
      "Fetching rating for HAL...\n",
      "Fetching rating for LMT...\n",
      "Fetching rating for AMZN...\n",
      "Fetching rating for CRM...\n",
      "Fetching rating for NOW...\n",
      "Fetching rating for CHTR...\n",
      "Fetching rating for TDS...\n",
      "Fetching rating for META...\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'stock_ratings.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 87\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo ratings were retrieved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 87\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 77\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_ratings:\n\u001b[0;32m     75\u001b[0m     keys \u001b[38;5;241m=\u001b[39m all_ratings[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstock_ratings.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m output_file:\n\u001b[0;32m     78\u001b[0m         dict_writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mDictWriter(output_file, keys)\n\u001b[0;32m     79\u001b[0m         dict_writer\u001b[38;5;241m.\u001b[39mwriteheader()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'stock_ratings.csv'"
     ]
    }
   ],
   "source": [
    "import functions_framework\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from textblob import TextBlob\n",
    "from google.cloud import bigquery\n",
    "import pytz\n",
    "\n",
    "\n",
    "# Replace with your NewsAPI key\n",
    "api_key = 'afc3fe9ac08745439bf521cb5b974fbc'\n",
    "\n",
    "# BigQuery configuration\n",
    "project_id = \"trendsense\"\n",
    "dataset_id = \"market_data\"\n",
    "table_id = \"News_News_Extract\"\n",
    "\n",
    "# List of tickers to search news for\n",
    "tickers = [\n",
    "    'AAPL', 'GOOGL', 'MSFT', 'ASTS', 'PTON', 'GSAT', 'PLTR', 'SMR', 'ACHR',\n",
    "    'BWXT', 'ARBK', 'AMD', 'NVDA', 'GME', 'MU', 'TSLA', 'NFLX', 'ZG',\n",
    "    'AVGO', 'SMCI', 'GLW', 'HAL', 'LMT', 'AMZN', 'CRM', 'NOW', 'CHTR', 'TDS', 'META'\n",
    "]\n",
    "\n",
    "# Get yesterday's date in ISO format\n",
    "yesterday = datetime.utcnow() - timedelta(days=1)\n",
    "yesterday_str = yesterday.strftime('%Y-%m-%d')\n",
    "\n",
    "# Function for TextBlob sentiment analysis\n",
    "def textblob_sentiment(text):\n",
    "    if text:\n",
    "        return TextBlob(text).sentiment.polarity  # Sentiment polarity from -1 to 1\n",
    "    return 0\n",
    "\n",
    "# Function to fetch market news for a specific date\n",
    "def get_market_news(ticker, date):\n",
    "    url = (\n",
    "        f'https://newsapi.org/v2/everything?q={ticker}&from={date}&to={date}&sortBy=publishedAt&apiKey={api_key}'\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get('articles', [])\n",
    "    elif response.status_code == 429:\n",
    "        print(f\"Rate limit exceeded for {ticker}, retrying after delay...\")\n",
    "        time.sleep(5)\n",
    "        return []\n",
    "    else:\n",
    "        print(f\"Error fetching data for {ticker}: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Function to save data to BigQuery\n",
    "def save_to_bigquery(data, project_id, dataset_id, table_id):\n",
    "    from google.cloud import bigquery\n",
    "    \n",
    "    client = bigquery.Client()\n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "    \n",
    "    # Define schema if table doesn't exist\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"ticker\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"title\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"summary\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"summary_textblob_sentiment\", \"FLOAT\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"publisher\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"link\", \"STRING\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"publish_date\", \"TIMESTAMP\", mode=\"NULLABLE\"),\n",
    "        bigquery.SchemaField(\"source\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    ]\n",
    "    \n",
    "    # Check if the table exists\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "    except Exception:\n",
    "        print(f\"Table {table_ref} does not exist. Creating it...\")\n",
    "        table = bigquery.Table(table_ref, schema=schema)\n",
    "        client.create_table(table)\n",
    "        print(f\"Table {table_ref} created.\")\n",
    "    \n",
    "    # Convert publish_date to datetime and then to MST\n",
    "    data['publish_date'] = pd.to_datetime(data['publish_date'], errors='coerce')\n",
    "    utc = pytz.utc\n",
    "    mst = pytz.timezone('US/Mountain')\n",
    "    data['publish_date'] = data['publish_date'].apply(\n",
    "        lambda x: x.astimezone(mst) if pd.notnull(x) else None\n",
    "    )\n",
    "    \n",
    "    # Ensure numeric values for sentiment\n",
    "    data['summary_textblob_sentiment'] = pd.to_numeric(data['summary_textblob_sentiment'], errors='coerce')\n",
    "\n",
    "    # Log data types to verify\n",
    "    print(\"DataFrame dtypes before uploading:\")\n",
    "    print(data.dtypes)\n",
    "\n",
    "    # Load data into BigQuery\n",
    "    job = client.load_table_from_dataframe(data, table_ref)\n",
    "    job.result()  # Wait for the load job to complete\n",
    "    print(f\"Data successfully saved to BigQuery table: {table_ref}\")\n",
    "    \n",
    "# Cloud Function Entry Point\n",
    "@functions_framework.http\n",
    "def main(request):\n",
    "    all_news = []\n",
    "    for ticker in tickers:\n",
    "        print(f\"Fetching news for {ticker} from {yesterday_str}...\")\n",
    "        articles = get_market_news(ticker, yesterday_str)\n",
    "\n",
    "        for article in articles:\n",
    "            title = article.get('title', '')\n",
    "            summary = article.get('description', '')\n",
    "\n",
    "            # Sentiment analysis using TextBlob\n",
    "            summary_textblob_sentiment = textblob_sentiment(summary)\n",
    "\n",
    "            # Article schema\n",
    "            news_entry = {\n",
    "                'ticker': ticker,\n",
    "                'title': title,\n",
    "                'summary': summary,\n",
    "                'summary_textblob_sentiment': summary_textblob_sentiment,\n",
    "                'publisher': article.get('source', {}).get('name', ''),\n",
    "                'link': article.get('url', ''),\n",
    "                'publish_date': article.get('publishedAt', ''),\n",
    "                'source': 'NewsAPI',  # Identify source\n",
    "            }\n",
    "            all_news.append(news_entry)\n",
    "\n",
    "        # Avoid rate limiting\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Convert data to a DataFrame\n",
    "    df = pd.DataFrame(all_news)\n",
    "\n",
    "    if not df.empty:\n",
    "        # Save to BigQuery\n",
    "        save_to_bigquery(df, project_id, dataset_id, table_id)\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": f\"Data saved to BigQuery table: {project_id}.{dataset_id}.{table_id}\",\n",
    "            \"total_articles\": len(all_news),\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"message\": \"No news articles found for yesterday.\",\n",
    "            \"total_articles\": 0,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\Bryce\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Bryce\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-11 10:21:53,724 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-distilbert/sentiment-en-mix-distillbert_4.pt not found in cache, downloading to C:\\Users\\Bryce\\AppData\\Local\\Temp\\tmpzt4pdo0y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 253M/253M [02:38<00:00, 1.67MB/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-11 10:24:33,105 copying C:\\Users\\Bryce\\AppData\\Local\\Temp\\tmpzt4pdo0y to cache at C:\\Users\\Bryce\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-11 10:24:33,666 removing temp file C:\\Users\\Bryce\\AppData\\Local\\Temp\\tmpzt4pdo0y\n",
      "Sentiment Analysis Results:\n",
      "TextBlob: 0.25625\n",
      "VADER: 0.7964\n",
      "BERT: {'label': 'POSITIVE', 'score': 0.9996393918991089}\n",
      "Flair: Sentence[44]: \"The company is making significant strides in the quantum computing field with its latest AI-powered calibration breakthrough. In collaboration with Quantum Machines, Rigetti successfully applied artificial intelligence to automate the calibration of a 9-qubit Novera Quantum Processing Unit (QPU).\" → POSITIVE (0.9998)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries (uncomment if needed)\n",
    "# !pip install textblob vaderSentiment transformers flair\n",
    "\n",
    "# Import libraries\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "\n",
    "# Input text\n",
    "text = (\n",
    "    \"The company is making significant strides in the quantum computing field \"\n",
    "    \"with its latest AI-powered calibration breakthrough. In collaboration with Quantum Machines, \"\n",
    "    \"Rigetti successfully applied artificial intelligence to automate the calibration of a 9-qubit \"\n",
    "    \"Novera Quantum Processing Unit (QPU).\"\n",
    ")\n",
    "\n",
    "# TextBlob Sentiment Analysis\n",
    "def analyze_textblob(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "# VADER Sentiment Analysis\n",
    "def analyze_vader(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores[\"compound\"]\n",
    "\n",
    "# BERT Sentiment Analysis\n",
    "def analyze_bert(text):\n",
    "    sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    return result\n",
    "\n",
    "# Flair Sentiment Analysis\n",
    "def analyze_flair(text):\n",
    "    classifier = TextClassifier.load(\"sentiment\")\n",
    "    sentence = Sentence(text)\n",
    "    classifier.predict(sentence)\n",
    "    sentiment_score = sentence.labels[0]\n",
    "    return sentiment_score\n",
    "\n",
    "# Run all analyses\n",
    "results = {\n",
    "    \"TextBlob\": analyze_textblob(text),\n",
    "    \"VADER\": analyze_vader(text),\n",
    "    \"BERT\": analyze_bert(text),\n",
    "    \"Flair\": analyze_flair(text),\n",
    "}\n",
    "\n",
    "# Print Results\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "for method, result in results.items():\n",
    "    print(f\"{method}: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "  Downloading flair-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting boto3>=1.20.27 (from flair)\n",
      "  Downloading boto3-1.35.78-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting conllu<5.0.0,>=4.0 (from flair)\n",
      "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Collecting deprecated>=1.2.13 (from flair)\n",
      "  Downloading Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting ftfy>=6.1.0 (from flair)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting gdown>=4.4.0 (from flair)\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.10.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flair) (0.26.5)\n",
      "Collecting langdetect>=1.0.9 (from flair)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     ------------------------------ ------- 786.4/981.5 kB 8.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- 981.5/981.5 kB 2.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: lxml>=4.8.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flair) (5.3.0)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flair) (3.9.2)\n",
      "Collecting more-itertools>=8.13.0 (from flair)\n",
      "  Downloading more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting mpld3>=0.3 (from flair)\n",
      "  Downloading mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pptree>=3.1 (from flair)\n",
      "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bryce\\appdata\\roaming\\python\\python312\\site-packages (from flair) (2.9.0.post0)\n",
      "Collecting pytorch-revgrad>=0.2.0 (from flair)\n",
      "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flair) (2024.9.11)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flair) (1.5.2)\n",
      "Collecting segtok>=1.5.11 (from flair)\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting sqlitedict>=2.0.0 (from flair)\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting tabulate>=0.8.10 (from flair)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flair) (2.5.1)\n",
      "Requirement already satisfied: tqdm>=4.63.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from flair) (4.66.5)\n",
      "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
      "  Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.18.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (4.47.0)\n",
      "Collecting wikipedia-api>=0.5.7 (from flair)\n",
      "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting semver<4.0.0,>=3.0.0 (from flair)\n",
      "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting bioc<3.0.0,>=2.0.0 (from flair)\n",
      "  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting jsonlines>=1.2.0 (from bioc<3.0.0,>=2.0.0->flair)\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting intervaltree (from bioc<3.0.0,>=2.0.0->flair)\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting docopt (from bioc<3.0.0,>=2.0.0->flair)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting botocore<1.36.0,>=1.35.78 (from boto3>=1.20.27->flair)\n",
      "  Downloading botocore-1.35.78-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.20.27->flair)\n",
      "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deprecated>=1.2.13->flair) (1.16.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\bryce\\appdata\\roaming\\python\\python312\\site-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown>=4.4.0->flair) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown>=4.4.0->flair) (3.16.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gdown>=4.4.0->flair) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\bryce\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.10.0->flair) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.10.0->flair) (4.12.2)\n",
      "Requirement already satisfied: six in c:\\users\\bryce\\appdata\\roaming\\python\\python312\\site-packages (from langdetect>=1.0.9->flair) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.2.3->flair) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.2.3->flair) (1.26.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.2.3->flair) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=2.2.3->flair) (3.1.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mpld3>=0.3->flair) (3.1.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.0.2->flair) (3.5.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (3.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch!=1.8,>=1.5.0->flair) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch!=1.8,>=1.5.0->flair) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bryce\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.63.0->flair) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.18.0->transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers<5.0.0,>=4.18.0->transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (0.4.5)\n",
      "Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair)\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[sentencepiece]<5.0.0,>=4.18.0->flair) (5.28.1)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from botocore<1.36.0,>=1.35.78->boto3>=1.20.27->flair) (2.2.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (24.2.0)\n",
      "Collecting accelerate>=0.26.0 (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair)\n",
      "  Downloading accelerate-1.2.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.6)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair) (2.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->mpld3>=0.3->flair) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown>=4.4.0->flair) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown>=4.4.0->flair) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown>=4.4.0->flair) (2024.8.30)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\bryce\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\bryce\\appdata\\roaming\\python\\python312\\site-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (6.0.0)\n",
      "Downloading flair-0.14.0-py3-none-any.whl (776 kB)\n",
      "   ---------------------------------------- 0.0/776.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 776.5/776.5 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading bioc-2.1-py3-none-any.whl (33 kB)\n",
      "Downloading boto3-1.35.78-py3-none-any.whl (139 kB)\n",
      "Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
      "Downloading Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "Downloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
      "Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
      "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading botocore-1.35.78-py3-none-any.whl (13.2 MB)\n",
      "   ---------------------------------------- 0.0/13.2 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 2.4/13.2 MB 12.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 5.0/13.2 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 7.3/13.2 MB 11.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 10.0/13.2 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.3/13.2 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.2 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.2/13.2 MB 9.2 MB/s eta 0:00:00\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
      "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 992.0/992.0 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading accelerate-1.2.0-py3-none-any.whl (336 kB)\n",
      "Building wheels for collected packages: langdetect, pptree, sqlitedict, wikipedia-api, docopt, intervaltree\n",
      "  Building wheel for langdetect (pyproject.toml): started\n",
      "  Building wheel for langdetect (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993251 sha256=caddf465fa8024e03b2a9405bbebc4f10df08ec9cd61ae679bc46fa5855efeef\n",
      "  Stored in directory: c:\\users\\bryce\\appdata\\local\\pip\\cache\\wheels\\c1\\67\\88\\e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "  Building wheel for pptree (pyproject.toml): started\n",
      "  Building wheel for pptree (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4611 sha256=64c107e63be823629d4a6308fa9248e230b5ee988862363a79f2d4ce4b322891\n",
      "  Stored in directory: c:\\users\\bryce\\appdata\\local\\pip\\cache\\wheels\\b0\\2d\\de\\37058114a8f07cfec75747cb46b864bc5c71b0e9e0e4cd0acd\n",
      "  Building wheel for sqlitedict (pyproject.toml): started\n",
      "  Building wheel for sqlitedict (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16903 sha256=ee84e521747b7b4e4396a95924e7c3d80e6304cc992200f233d2d117d43d71c5\n",
      "  Stored in directory: c:\\users\\bryce\\appdata\\local\\pip\\cache\\wheels\\7a\\6f\\21\\fc016aef45ffcabe27129a2252f061387cbf278d2086225a64\n",
      "  Building wheel for wikipedia-api (pyproject.toml): started\n",
      "  Building wheel for wikipedia-api (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14397 sha256=8324065891918b1cf2f2a4f0b83731aa3eec7e6558dec3bfe2db862382074157\n",
      "  Stored in directory: c:\\users\\bryce\\appdata\\local\\pip\\cache\\wheels\\48\\93\\2f\\978da1e445cf17606445f4b47fd8454250f5440d5a10c677e9\n",
      "  Building wheel for docopt (pyproject.toml): started\n",
      "  Building wheel for docopt (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13776 sha256=4926c6b7a4dce06f65671ba7559692959e2a1d86bc86e22388111dddf61fb0e9\n",
      "  Stored in directory: c:\\users\\bryce\\appdata\\local\\pip\\cache\\wheels\\1a\\bf\\a1\\4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "  Building wheel for intervaltree (pyproject.toml): started\n",
      "  Building wheel for intervaltree (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26128 sha256=b1b1ea45f93971496a6f6aed05fd573772269df3381b7d518a4ec841f3506fc2\n",
      "  Stored in directory: c:\\users\\bryce\\appdata\\local\\pip\\cache\\wheels\\65\\c3\\c3\\238bf93c243597857edd94ddb0577faa74a8e16e9585896e83\n",
      "Successfully built langdetect pptree sqlitedict wikipedia-api docopt intervaltree\n",
      "Installing collected packages: sqlitedict, sentencepiece, pptree, docopt, tabulate, semver, segtok, more-itertools, langdetect, jsonlines, jmespath, intervaltree, ftfy, deprecated, conllu, wikipedia-api, botocore, bioc, s3transfer, pytorch-revgrad, mpld3, gdown, accelerate, boto3, transformer-smaller-training-vocab, flair\n",
      "Successfully installed accelerate-1.2.0 bioc-2.1 boto3-1.35.78 botocore-1.35.78 conllu-4.5.3 deprecated-1.2.15 docopt-0.6.2 flair-0.14.0 ftfy-6.3.1 gdown-5.2.0 intervaltree-3.1.0 jmespath-1.0.1 jsonlines-4.0.0 langdetect-1.0.9 more-itertools-10.5.0 mpld3-0.5.10 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.10.4 segtok-1.5.11 semver-3.0.2 sentencepiece-0.2.0 sqlitedict-2.1.0 tabulate-0.9.0 transformer-smaller-training-vocab-0.4.0 wikipedia-api-0.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install flair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Financial Modeling Prep API key is not configured.\n",
      "\n",
      "Retrieved Ratings Summary:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "import json\n",
    "\n",
    "# Configuration - use environment variables or defaults for local testing\n",
    "API_KEY = os.environ.get('FINANCIAL_MODELING_PREP_API_KEY', 'KhbgwU29WSYBQlGkdkYjAomzvDQRVE0')\n",
    "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT', 'trendsense')\n",
    "DATASET_ID = os.environ.get('BIGQUERY_DATASET_ID', 'stock_data')\n",
    "TABLE_ID = os.environ.get('BIGQUERY_TABLE_ID', 'stock_data_ratings')\n",
    "\n",
    "# List of stock symbols\n",
    "SYMBOLS = [\n",
    "    'AAPL', 'GOOGL', 'MSFT', 'ASTS', 'PTON', 'GSAT', 'PLTR', 'SMR', 'ACHR',\n",
    "    'BWXT', 'ARBK', 'AMD', 'NVDA', 'GME', 'MU', 'TSLA', 'NFLX', 'ZG',\n",
    "    'AVGO', 'SMCI', 'GLW', 'HAL', 'LMT', 'AMZN', 'CRM', 'NOW', 'CHTR', 'TDS', 'META'\n",
    "]\n",
    "\n",
    "def get_company_rating(symbol, api_key):\n",
    "    \"\"\"\n",
    "    Fetch company rating for a given stock symbol from Financial Modeling Prep API\n",
    "    \n",
    "    Args:\n",
    "        symbol (str): Stock symbol to fetch rating for\n",
    "        api_key (str): API key for Financial Modeling Prep\n",
    "    \n",
    "    Returns:\n",
    "        dict or None: Parsed rating data or None if fetch fails\n",
    "    \"\"\"\n",
    "    if not api_key or api_key == 'KhbgwU29WSYBQlGkdkYjAomzvDQRVE0':\n",
    "        print(f\"Invalid API key. Please provide a valid Financial Modeling Prep API key.\")\n",
    "        return None\n",
    "\n",
    "    url = f\"https://financialmodelingprep.com/api/v3/rating/{symbol}?apikey={api_key}\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        \n",
    "        # Log detailed error information for debugging\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Rating fetch failed for {symbol}. Status: {response.status_code}\")\n",
    "            print(f\"Response content: {response.text}\")\n",
    "            return None\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        if not data:\n",
    "            print(f\"No rating data available for {symbol}\")\n",
    "            return None\n",
    "        \n",
    "        # Extract the first result\n",
    "        rating_data = data[0]\n",
    "        return {\n",
    "            'symbol': symbol,\n",
    "            'fetch_timestamp': datetime.utcnow().isoformat(),\n",
    "            'date': rating_data.get('date', 'No date available'),\n",
    "            'overall_rating': rating_data.get('rating'),\n",
    "            'recommendation': rating_data.get('ratingRecommendation'),\n",
    "            'rating_score': rating_data.get('ratingScore', 0.0),\n",
    "            'dcf_score': rating_data.get('ratingDetailsDCFScore', 0.0),\n",
    "            'dcf_recommendation': rating_data.get('ratingDetailsDCFRecommendation'),\n",
    "            'roe_score': rating_data.get('ratingDetailsROEScore', 0.0),\n",
    "            'roe_recommendation': rating_data.get('ratingDetailsROERecommendation'),\n",
    "            'roa_score': rating_data.get('ratingDetailsROAScore', 0.0),\n",
    "            'roa_recommendation': rating_data.get('ratingDetailsROARecommendation'),\n",
    "            'pe_score': rating_data.get('ratingDetailsPEScore', 0.0),\n",
    "            'pe_recommendation': rating_data.get('ratingDetailsPERecommendation'),\n",
    "            'pb_score': rating_data.get('ratingDetailsPBScore', 0.0),\n",
    "            'pb_recommendation': rating_data.get('ratingDetailsPBRecommendation')\n",
    "        }\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Network error for {symbol}: {e}\")\n",
    "        return None\n",
    "    except ValueError as e:\n",
    "        print(f\"JSON parsing error for {symbol}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {symbol}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_and_save_stock_ratings(api_key, project_id, dataset_id, table_id):\n",
    "    \"\"\"\n",
    "    Fetch stock ratings and optionally save to BigQuery or local file\n",
    "    \n",
    "    Args:\n",
    "        api_key (str): Financial Modeling Prep API key\n",
    "        project_id (str): Google Cloud Project ID\n",
    "        dataset_id (str): BigQuery dataset ID\n",
    "        table_id (str): BigQuery table ID\n",
    "    \n",
    "    Returns:\n",
    "        list: List of fetched stock ratings\n",
    "    \"\"\"\n",
    "    # Validate configuration\n",
    "    if not api_key or api_key == 'KhbgwU29WSYBQlGkdkYjAomzvDQRVE0':\n",
    "        print(\"ERROR: Financial Modeling Prep API key is not configured.\")\n",
    "        return []\n",
    "\n",
    "    # Collect ratings\n",
    "    ratings_to_save = []\n",
    "    error_symbols = []\n",
    "\n",
    "    # Fetch ratings for each symbol\n",
    "    for symbol in SYMBOLS:\n",
    "        try:\n",
    "            rating = get_company_rating(symbol, api_key)\n",
    "            \n",
    "            if rating:\n",
    "                ratings_to_save.append(rating)\n",
    "            else:\n",
    "                error_symbols.append(symbol)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {symbol}: {e}\")\n",
    "            error_symbols.append(symbol)\n",
    "\n",
    "    # Save results locally (JSON)\n",
    "    if ratings_to_save:\n",
    "        # Save to local JSON file\n",
    "        output_file = 'stock_ratings_output.json'\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(ratings_to_save, f, indent=2)\n",
    "        print(f\"Saved {len(ratings_to_save)} stock ratings to {output_file}\")\n",
    "\n",
    "        # Optionally save to BigQuery if credentials are set up\n",
    "        try:\n",
    "            # Only attempt BigQuery if running in a Google Cloud environment\n",
    "            client = bigquery.Client(project=project_id)\n",
    "            \n",
    "            # Prepare the table reference\n",
    "            dataset_ref = client.dataset(dataset_id)\n",
    "            table_ref = dataset_ref.table(table_id)\n",
    "            \n",
    "            # Insert rows into BigQuery\n",
    "            errors = client.insert_rows_json(table_ref, ratings_to_save)\n",
    "            \n",
    "            if errors:\n",
    "                print(f\"Partial failure inserting rows. Symbols with errors: {error_symbols}\")\n",
    "            else:\n",
    "                print(f\"Successfully inserted {len(ratings_to_save)} stocks to BigQuery\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"BigQuery insertion error: {e}\")\n",
    "            print(\"Continuing with local file save...\")\n",
    "\n",
    "    else:\n",
    "        print('No ratings could be retrieved')\n",
    "\n",
    "    return ratings_to_save\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run the stock ratings fetch\n",
    "    \"\"\"\n",
    "    # You can replace 'YOUR_API_KEY_HERE' with your actual API key\n",
    "    ratings = fetch_and_save_stock_ratings(\n",
    "        api_key=API_KEY, \n",
    "        project_id=PROJECT_ID, \n",
    "        dataset_id=DATASET_ID, \n",
    "        table_id=TABLE_ID\n",
    "    )\n",
    "    \n",
    "    # Print out basic info about retrieved ratings\n",
    "    print(\"\\nRetrieved Ratings Summary:\")\n",
    "    for rating in ratings:\n",
    "        print(f\"{rating['symbol']}: {rating.get('recommendation', 'No recommendation')}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
